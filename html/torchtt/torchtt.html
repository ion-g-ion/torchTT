<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>torchtt.torchtt API documentation</title>
<meta name="description" content="Basic class for TT decomposition.
It contains the base TT class as well as additional functions." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>torchtt.torchtt</code></h1>
</header>
<section id="section-intro">
<p>Basic class for TT decomposition.
It contains the base TT class as well as additional functions.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Basic class for TT decomposition.
It contains the base TT class as well as additional functions.

&#34;&#34;&#34;

import torch as tn
import torch.nn.functional as tnf
from torchtt._decomposition import mat_to_tt, to_tt, lr_orthogonal, round_tt, rl_orthogonal, QR
from  torchtt._division import amen_divide
import numpy as np
import math 
from torchtt._dmrg import dmrg_matvec
from torchtt._aux_ops import apply_mask, dense_matvec, bilinear_form_aux
from torchtt.errors import *

class TT():
    
    def __init__(self, source, shape=None, eps=1e-10, rmax=2048):
        &#34;&#34;&#34;
        Constructor of the TT class. Can convert full tensor in the TT-format (from `torch.tensor` or `numpy.array`).
        In the case of tensor operators of full shape `M1 x ... Md x N1 x ... x Nd`, the shape must be specified as a list of tuples `[(M1,N1),...,(Md,Nd)]`.
        A TT-object can also be computed from cores if the list of cores is passed as argument.
        If None is provided, an empty tensor is created.
        
        \(\\mathsf{x}=\\sum\\limits_{r_1...r_{d-1}=1}^{R_1,...,R_{d-1}} \\mathsf{x}^{(1)}_{1i_1r_1}\\cdots\\mathsf{x}^{(d)}_{r_{d-1}i_d1}\)
   
        Args:
            source (torch.tensor ot list[torch.tensor] or numpy.array or None): the input tensor in full format or the cores. If a torch.tensor or numpy array is provided
            shape (list[int] or list[tuple[int]], optional): the shape (if it differs from the one provided). For the TT-matrix case is mandatory. Defaults to None.
            eps (float, optional): tolerance of the TT approximation. Defaults to 1e-10.
            rmax (int or list[int], optional): maximum rank (either a list of integer or an integer). Defaults to 1000.

        Raises:
            RankMismatch: Ranks of the given cores do not match (chace the spaes of the cores).
            InvalidArguments: Invalid input: TT-cores have to be either 4d or 3d.
            InvalidArguments: Check the ranks and the mode size.
            NotImplementedError: Function only implemented for torch tensors, numpy arrays, list of cores as torch tensors and None
        
        Examples:
            import torchtt
            import torch
            x = torch.reshape(torch.arange(0,128,dtype = torch.float64),[8,4,4])
            xtt = torchtt.TT(x)
            ytt = torchtt.TT(torch.squeeze(x),[8,4,4])
            A = torch.reshape(torch.arange(0,20160,dtype = torch.float64),[3,5,7,4,6,8])
            Att = torchtt.TT(A,[(3,4),(5,6),(7,8)])
            print(Att)        
        
        &#34;&#34;&#34;
       
        
        if source is None:
            # empty TT
            self.cores = []
            self.M = []
            self.N = []
            self.R = [1,1]
            self.is_ttm = False
            
        elif isinstance(source, list):
            # tt cores were passed directly
            
            # check if sizes are consistent
            prev = 1
            N = []
            M = []
            R = [source[0].shape[0]]
            d = len(source)
            for i in range(len(source)):
                s = source[i].shape
                
                if s[0] != R[-1]:
                    raise RankMismatch(&#34;Ranks of the given cores do not match (chace the spaes of the cores).&#34;)
                if len(s) == 3:
                    R.append(s[2])
                    N.append(s[1])
                elif len(s)==4:
                    R.append(s[3])
                    M.append(s[1])
                    N.append(s[2])
                else:
                    raise InvalidArguments(&#34;Invalid input: TT-cores have to be either 4d or 3d.&#34;)
            
            if len(N) != d or len(R) != d+1 or R[0] != 1 or R[-1] != 1 or (len(M)!=0 and len(M)!=len(N)) :
                raise InvalidArguments(&#34;Check the ranks and the mode size.&#34;)
            
            self.cores = source
            self.R = R
            self.N = N
            if len(M) == len(N):
                self.M = M
                self.is_ttm = True
            else:
                self.is_ttm = False
            self.shape = [ (m,n) for m,n in zip(self.M,self.N) ] if self.is_ttm else [n for n in self.N]     

        elif tn.is_tensor(source):
            if shape == None:
                # no size is given. Deduce it from the tensor. No TT-matrix in this case.
                self.N = list(source.shape)
                if len(self.N)&gt;1:
                    self.cores, self.R = to_tt(source,self.N,eps,rmax,is_sparse=False)
                else:    
                    self.cores = [tn.reshape(source,[1,self.N[0],1])]
                    self.R = [1,1]
                self.is_ttm = False
            elif isinstance(shape,list) and isinstance(shape[0],tuple):
                # if the size contains tuples, we have a TT-matrix.
                if len(shape) &gt; 1:
                    self.M = [s[0] for s in shape]
                    self.N = [s[1] for s in shape]
                    self.cores, self.R = mat_to_tt(source, self.M, self.N, eps, rmax)
                    self.is_ttm = True
                else:
                    self.M = [shape[0][0]]
                    self.N = [shape[0][1]]
                    self.cores, self.R = [tn.reshape(source,[1,shape[0][0],shape[0][1],1])], [1,1]
                    self.is_ttm = True
            else:
                # TT-decomposition with prescribed size
                # perform reshape first
                self.N = shape
                self.cores, self.R = to_tt(tn.reshape(source,shape),self.N,eps,rmax,is_sparse=False)
                self.is_ttm = False
            self.shape = [ (m,n) for m,n in zip(self.M,self.N) ] if self.is_ttm else [n for n in self.N]     

        elif isinstance(source, np.ndarray):
            source = tn.tensor(source) 
                    
            if shape == None:
                # no size is given. Deduce it from the tensor. No TT-matrix in this case.
                self.N = list(source.shape)
                if len(self.N)&gt;1:
                    self.cores, self.R = to_tt(source,self.N,eps,rmax,is_sparse=False)
                else:    
                    self.cores = [tn.reshape(source,[1,self.N[0],1])]
                    self.R = [1,1]
                self.is_ttm = False
            elif isinstance(shape,list) and isinstance(shape[0],tuple):
                # if the size contains tuples, we have a TT-matrix.
                self.M = [s[0] for s in shape]
                self.N = [s[1] for s in shape]
                self.cores, self.R = mat_to_tt(source, self.M, self.N, eps, rmax)
                self.is_ttm = True
            else:
                # TT-decomposition with prescribed size
                # perform reshape first
                self.N = shape
                self.cores, self.R = to_tt(tn.reshape(source,shape),self.N,eps,rmax,is_sparse=False)
                self.is_ttm = False
            self.shape = [ (m,n) for m,n in zip(self.M,self.N) ] if self.is_ttm else [n for n in self.N]     
        else:
            raise NotImplementedError(&#34;Function only implemented for torch tensors, numpy arrays, list of cores as torch tensors and None.&#34;)

    def cuda(self, device = None):
        &#34;&#34;&#34;
        Return a torchtt.TT object on the CUDA device by cloning all the cores on the GPU.

        Args:
            device (torch.device, optional): The CUDA device (None for CPU). Defaults to None.

        Returns:
            TT-oject: The TT-object. The TT-cores are on CUDA.
        &#34;&#34;&#34;
         
        t = TT(None)
        t.N = self.N.copy()
        t.R = self.R.copy()
        t.is_ttm = self.is_ttm
        if self.is_ttm:
            t.M = self.M.copy()
        t.cores = [ c.cuda(device) for c in self.cores]

        return t

    def cpu(self):
        &#34;&#34;&#34;
        Retrive the cores from the GPU.

        Returns:
            TT-object: The TT-object on CPU.
        &#34;&#34;&#34;

        t = TT(None)
        t.N = self.N.copy()
        t.R = self.R.copy()
        t.is_ttm = self.is_ttm
        if self.is_ttm:
            t.M = self.M.copy()
        t.cores = [ c.cpu() for c in self.cores]

        return t

    def is_cuda(self):
        &#34;&#34;&#34;
        Return True if the tensor is on GPU.

        Returns:
            bool: Is the torchtt.TT on GPU or not.
        &#34;&#34;&#34;
        return all([c.is_cuda for c in self.core])

    
    def to(self, device = None, dtype = None):
        &#34;&#34;&#34;
        Moves the TT instance to the given device with the given dtype.

        Args:
            device (torch.device, optional): The desired device. If none is provided, the device is the CPU. Defaults to None.
            dtype (torch.dtype, optional): The desired dtype (torch.float64, torch.float32,...). If none is provided the dtype is not changed. Defaults to None.
        &#34;&#34;&#34;
        t = TT(None)
        t.N = self.N.copy()
        t.R = self.R.copy()
        t.is_ttm = self.is_ttm
        if self.is_ttm:
            t.M = self.M.copy()
        t.cores = [ c.to(device=device,dtype=dtype) for c in self.cores]

        return t

    def detach(self):
        &#34;&#34;&#34;
        Detaches the TT tensor. Similar to torch.tensor.detach().

        Returns:
            torchtt.TT: the detached tensor.
        &#34;&#34;&#34;
        return TT([c.detach() for c in self.cores])
        
    def clone(self):
        &#34;&#34;&#34;
        Clones the torchtt.TT instance. Similat to torch.tensor.clone().

        Returns:
            torchtt.TT: the cloned TT object.
        &#34;&#34;&#34;
        return TT([c.clone() for c in self.cores]) 

    def full(self):       
        &#34;&#34;&#34;
        Return the full tensor.
        In case of a TTM, the result has the shape M1 x M2 x ... x Md x N1 x N2 x ... x Nd.

        Returns:
            torch.tensor: the full tensor.
        &#34;&#34;&#34;
        if self.is_ttm:
            # the case of tt-matrix
            tfull = self.cores[0][0,:,:,:]
            for i in  range(1,len(self.cores)-1) :
                tfull = tn.einsum(&#39;...i,ijkl-&gt;...jkl&#39;,tfull,self.cores[i])
            if len(self.N) != 1:
                tfull = tn.einsum(&#39;...i,ijk-&gt;...jk&#39;,tfull,self.cores[-1][:,:,:,0])
                tfull = tn.permute(tfull,list(np.arange(len(self.N))*2)+list(np.arange(len(self.N))*2+1))
            else:
                tfull = tfull[:,:,0]
        else:
            # the case of a normal tt
            tfull = self.cores[0][0,:,:]
            for i in  range(1,len(self.cores)-1) :
                tfull = tn.einsum(&#39;...i,ijk-&gt;...jk&#39;,tfull,self.cores[i])
            if len(self.N) != 1:
                tfull = tn.einsum(&#39;...i,ij-&gt;...j&#39;,tfull,self.cores[-1][:,:,0])
            else:
                tfull = tn.squeeze(tfull)
        return tfull
    
    def numpy(self):
        &#34;&#34;&#34;
        Return the full tensor as a numpy.array.
        In case of a TTM, the result has the shape M1 x M2 x ... x Md x N1 x N2 x ... x Nd.
        If it is involved in an AD graph, an error will occur.
        
        Returns:
            numpy.array: the full tensor in numpy.
        &#34;&#34;&#34;
        return self.full().cpu().numpy()
    
    def __repr__(self):
        &#34;&#34;&#34;
        Show the information as a string

        Returns:
            string: the string representation of a torchtt.TT
        &#34;&#34;&#34;
        
        if self.is_ttm:
            output = &#39;TT-matrix&#39; 
            output += &#39; with sizes and ranks:\n&#39;
            output += &#39;M = &#39; + str(self.M) + &#39;\nN = &#39; + str(self.N) + &#39;\n&#39;
            output += &#39;R = &#39; + str(self.R) + &#39;\n&#39;
            output += &#39;Device: &#39;+str(self.cores[0].device)+&#39;, dtype: &#39;+str(self.cores[0].dtype)+&#39;\n&#39;
            entries = sum([tn.numel(c)  for c in self.cores])
            output += &#39;#entries &#39; + str(entries) +&#39; compression &#39; + str(entries/np.prod(np.array(self.N,dtype=np.float64)*np.array(self.M,dtype=np.float64))) +  &#39;\n&#39;
        else:
            output = &#39;TT&#39;
            output += &#39; with sizes and ranks:\n&#39;
            output += &#39;N = &#39; + str(self.N) + &#39;\n&#39;
            output += &#39;R = &#39; + str(self.R) + &#39;\n\n&#39;
            output += &#39;Device: &#39;+str(self.cores[0].device)+&#39;, dtype: &#39;+str(self.cores[0].dtype)+&#39;\n&#39;
            entries = sum([tn.numel(c) for c in self.cores])
            output += &#39;#entries &#39; + str(entries) +&#39; compression &#39;  + str(entries/np.prod(np.array(self.N,dtype=np.float64))) + &#39;\n&#39;
        
        return output
    
    def __radd__(self,other):
        &#34;&#34;&#34;
        Addition in the TT format. Implements the &#34;+&#34; operator. This function is called in the case a non-torchtt.TT object is added to the left.

        Args:
            other (int or float or torch.tensor scalar): the left operand.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        return self.__add__(other)

    def __add__(self,other):
        &#34;&#34;&#34;
        Addition in the TT format. Implements the &#34;+&#34; operator. The following type pairs are supported:
            - both operands are TT-tensors.
            - both operands are TT-matrices.
            - first operand is a TT-tensor or a TT-matrix and the second is a scalar (either torch.tensor scalar or int or float).

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): second operand.
            
        Raises:
            ShapeMismatch: Dimension mismatch.
            IncompatibleTypes: Addition between a tensor and a matrix is not defined.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;

        if np.isscalar(other) or ( tn.is_tensor(other) and tn.numel(other) == 1):
            # the second term is a scalar
            cores =  []
            
            for i in range(len(self.N)):
                if self.is_ttm:
                    pad1 = (0,0 if i == len(self.N)-1 else 1 , 0,0 , 0,0 , 0,0 if i==0 else 1)
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0,0 , 0 if i==0 else self.R[i],0)
                    othr = tn.ones([1,1,1,1],dtype=self.cores[i].dtype) * (other if i ==0 else 1)
                else:
                    pad1 = (0,0 if i == len(self.N)-1 else 1 , 0,0 , 0,0 if i==0 else 1)
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0 if i==0 else self.R[i],0)
                    othr = tn.ones([1,1,1],dtype=self.cores[i].dtype) * (other if i ==0 else 1)
                

                cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(othr,pad2))

                
            result = TT(cores)
        elif isinstance(other,TT):
        #second term is TT object 
            if self.is_ttm and other.is_ttm:
                # both are TT-matrices
                if self.M != self.M or self.N != self.N:
                    raise ShapeMismatch(&#39;Dimension mismatch.&#39;)
                    
                cores = []
                for i in range(len(self.N)):
                    pad1 = (0,0 if i == len(self.N)-1 else other.R[i+1], 0,0 , 0,0 , 0,0 if i==0 else other.R[i])
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0,0 , 0 if i==0 else self.R[i],0)
                    cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(other.cores[i],pad2))
                    
                result = TT(cores)
                
            elif self.is_ttm==False and other.is_ttm==False:
                # normal tensors in TT format.
                if self.N != self.N:
                    raise ShapeMismatch(&#39;Dimension mismatch.&#39;)
                    
                cores = []
                for i in range(len(self.N)):
                    pad1 = (0,0 if i == len(self.N)-1 else other.R[i+1] , 0,0 , 0,0 if i==0 else other.R[i])
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0 if i==0 else self.R[i],0)
                    cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(other.cores[i],pad2))
                    
                    
                result = TT(cores)
                
                
            else:
                # incompatible types 
                raise IncompatibleTypes(&#39;Addition between a tensor and a matrix is not defined.&#39;)
        else:
            InvalidArguments(&#39;Second term is incompatible.&#39;)
            
        return result
    
    def __rsub__(self,other):
        &#34;&#34;&#34;
        Subtract 2 tensors in the TT format. Implements the &#34;-&#34; operator.  

        Args:
            other (int or float or torch.tensor with 1 element): the first operand.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        T = self.__sub__(other)
        T.cores[0] = -T.cores[0]
        return T
    
    def __sub__(self,other):
        &#34;&#34;&#34;
        Subtract 2 tensors in the TT format. Implements the &#34;-&#34; operator.
        Possible second operands are: torchtt.TT, float, int, torch.tensor with 1 element.

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the second operand.

        Raises:
            ShapeMismatch: Both dimensions of the TT matrix should be equal.
            ShapeMismatch: Dimension mismatch.
            IncompatibleTypes: Addition between a tensor and a matrix is not defined.
            InvalidArguments: Second term is incompatible (must be either torchtt.TT or int or float or torch.tensor with 1 element).

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        if np.isscalar(other) or ( tn.is_tensor(other) and other.shape == []):
            # the second term is a scalar
            cores =  []
            
            for i in range(len(self.N)):
                if self.is_ttm:
                    pad1 = (0,0 if i == len(self.N)-1 else 1 , 0,0 , 0,0 , 0,0 if i==0 else 1)
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0,0 , 0 if i==0 else self.R[i],0)
                    othr = tn.ones([1,1,1,1],dtype=self.cores[i].dtype) * (-other if i ==0 else 1)
                else:
                    pad1 = (0,0 if i == len(self.N)-1 else 1 , 0,0 , 0,0 if i==0 else 1)
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0 if i==0 else self.R[i],0)
                    othr = tn.ones([1,1,1],dtype=self.cores[i].dtype) * (-other if i ==0 else 1)
                cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(othr,pad2))
            result = TT(cores)

        elif isinstance(other,TT):
        #second term is TT object 
            if self.is_ttm and other.is_ttm:
                # both are TT-matrices
                if self.M != self.M or self.N != self.N:
                    raise ShapeMismatch(&#39;Both dimensions of the TT matrix should be equal.&#39;)
                    
                cores = []
                for i in range(len(self.N)):
                    pad1 = (0,0 if i == len(self.N)-1 else other.R[i+1] , 0,0 , 0,0 , 0,0 if i==0 else other.R[i])
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0,0 , 0 if i==0 else self.R[i],0)
                    cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(-other.cores[i] if i==0 else other.cores[i],pad2))
                    
                result = TT(cores)
                
            elif self.is_ttm==False and other.is_ttm==False:
                # normal tensors in TT format.
                if self.N != self.N:
                    raise ShapeMismatch(&#39;Dimension mismatch.&#39;)
                    
                cores = []
                for i in range(len(self.N)):
                    pad1 = (0,0 if i == len(self.N)-1 else other.R[i+1] , 0,0 , 0,0 if i==0 else other.R[i])
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0 if i==0 else self.R[i],0)
                    cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(-other.cores[i] if i==0 else other.cores[i],pad2))
                    
                    
                result = TT(cores)
                
                
            else:
                # incompatible types 
                raise IncompatibleTypes(&#39;Addition between a tensor and a matrix is not defined.&#39;)
        else:
            InvalidArguments(&#39;Second term is incompatible (must be either torchtt.TT or int or float or torch.tensor with 1 element).&#39;)
            
        return result
    
    def __rmul__(self,other):
        &#34;&#34;&#34;
        Elementwise multiplication in the TT format.
        This implements the &#34;*&#34; operator when the left operand is not torchtt.TT.
        Following are supported:
         - TT tensor and TT tensor
         - TT matrix and TT matrix
         - TT tensor and scalar(int, float or torch.tensor scalar)

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the second operand.

        Raises:
            ShapeMismatch: Shapes must be equal.
            IncompatibleTypes: Second operand must be the same type as the fisrt (both should be either TT matrices or TT tensors).
            InvalidArguments: Second operand must be of type: torchtt.TT, float, int of torch.tensor.

        Returns:
            torchtt.TT: [description]
        &#34;&#34;&#34;
        
        return self.__mul__(other)
        
    def __mul__(self,other):
        &#34;&#34;&#34;
        Elementwise multiplication in the TT format.
        This implements the &#34;*&#34; operator.
        Following are supported:
         - TT tensor and TT tensor
         - TT matrix and TT matrix
         - TT tensor and scalar(int, float or torch.tensor scalar)

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the second operand.

        Raises:
            ShapeMismatch: Shapes must be equal.
            IncompatibleTypes: Second operand must be the same type as the fisrt (both should be either TT matrices or TT tensors).
            InvalidArguments: Second operand must be of type: torchtt.TT, float, int of torch.tensor.

        Returns:
            torchtt.TT: [description]
        &#34;&#34;&#34;
       
        # elementwise multiplication
        if isinstance(other, TT):
            if self.is_ttm and other.is_ttm:
                if self.N != other.N or self.M != other.M:
                    raise ShapeMismatch(&#39;Shapes must be equal.&#39;) 
                    
                cores_new = []
                
                for i in range(len(self.cores)):
                    core = tn.reshape(tn.einsum(&#39;aijb,mijn-&gt;amijbn&#39;,self.cores[i],other.cores[i]),[self.R[i]*other.R[i],self.M[i],self.N[i],self.R[i+1]*other.R[i+1]])
                    cores_new.append(core)
    
            elif self.is_ttm == False and other.is_ttm == False:
                if self.N != other.N:
                    raise ShapeMismatch(&#39;Shapes must be equal.&#39;) 
                    
                cores_new = []
                
                for i in range(len(self.cores)):
                    core = tn.reshape(tn.einsum(&#39;aib,min-&gt;amibn&#39;,self.cores[i],other.cores[i]),[self.R[i]*other.R[i],self.N[i],self.R[i+1]*other.R[i+1]])
                    cores_new.append(core)
            else:
                raise IncompatibleTypes(&#39;Second operand must be the same type as the fisrt (both should be either TT matrices or TT tensors).&#39;)
        elif isinstance(other,int) or isinstance(other,float) or isinstance(other,tn.tensor):
            cores_new = [c+0 for c in self.cores]
            cores_new[0] *= other
        else:
            raise InvalidArguments(&#39;Second operand must be of type: TT, float, int of tensorflow Tensor.&#39;)
        result = TT(cores_new)            
        return result
        
    def __matmul__(self,other):
        &#34;&#34;&#34;
        Matrix-vector multiplication in TT-format
        Suported operands:
            - TT-matrix @ TT-tensor -&gt; TT-tensor: y_i = A_ij * x_j
            - TT-tensor @ TT-matrix -&gt; TT-tensor: y_j = x_i * A_ij 
            - TT-matrix @ TT-matrix -&gt; TT-matrix: Y_ij = A_ik * B_kj
            - TT-matrix @ torch.tensor -&gt; torch.tensor: y_bi = A_ij * x_bj 
        In the last case, the multiplication is performed along the last modes and a full torch.tensor is returned.

        Args:
            other (torchtt.TT or torch.tensor): the second operand.

        Raises:
            ShapeMismatch: Shapes do not match.
            InvalidArguments: Wrong arguments.

        Returns:
            torchtt.TT or torch.tensor: the result. Can be full tensor if the second operand is full tensor.
        &#34;&#34;&#34;
     
        if self.is_ttm and tn.is_tensor(other):
            if self.N != list(other.shape)[-len(self.N):]:
                raise ShapeMismatch(&#34;Shapes do not match.&#34;)
            result = dense_matvec(self.cores,other) 
            return result

        elif self.is_ttm and other.is_ttm == False:
            # matrix-vector multiplication
            if self.N != other.N:
                raise ShapeMismatch(&#34;Shapes do not match.&#34;)
                
            cores_new = []
            
            for i in range(len(self.cores)):
                core = tn.reshape(tn.einsum(&#39;ijkl,mkp-&gt;imjlp&#39;,self.cores[i],other.cores[i]),[self.cores[i].shape[0]*other.cores[i].shape[0],self.cores[i].shape[1],self.cores[i].shape[3]*other.cores[i].shape[2]])
                cores_new.append(core)
            
            
        elif self.is_ttm and other.is_ttm:
            # multiplication between 2 TT-matrices
            if self.N != other.M:
                raise ShapeMismatch(&#34;Shapes do not match.&#34;)
                
            cores_new = []
            
            for i in range(len(self.cores)):
                core = tn.reshape(tn.einsum(&#39;ijkl,mknp-&gt;imjnlp&#39;,self.cores[i],other.cores[i]),[self.cores[i].shape[0]*other.cores[i].shape[0],self.cores[i].shape[1],other.cores[i].shape[2],self.cores[i].shape[3]*other.cores[i].shape[3]])
                cores_new.append(core)
        elif self.is_ttm == False and other.is_ttm:
            # vector-matrix multiplication
            if self.N != other.M:
                raise ShapeMismatch(&#34;Shapes do not match.&#34;)
                
            cores_new = []
            
            for i in range(len(self.cores)):
                core = tn.reshape(tn.einsum(&#39;mkp,ikjl-&gt;imjlp&#39;,self.cores[i],other.cores[i]),[self.cores[i].shape[0]*other.cores[i].shape[0],other.cores[i].shape[2],self.cores[i].shape[2]*other.cores[i].shape[3]])
                cores_new.append(core)
        else:
            raise InvalidArguments(&#34;Wrong arguments.&#34;)
            
        result = TT(cores_new)
        return result

    def fast_matvec(self,other, eps = 1e-12, nswp = 20, verb = False):
        &#34;&#34;&#34;
        Fast matrix vector multiplication A@x using DMRG iterations. Faster than traditional matvec + rounding.

        Args:
            other (torchtt.TT): the TT tensor.
            eps (float, optional): relative accuracy for DMRG. Defaults to 1e-12.
            nswp (int, optional): number of DMRG iterations. Defaults to 40.
            verb (bool, optional): show info for debug. Defaults to False.

        Raises:
            InvalidArguments: Second operand has to be TT object.
            IncompatibleTypes: First operand should be a TT matrix and second a TT vector.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        if not isinstance(other,TT):
            raise InvalidArguments(&#39;Second operand has to be TT object.&#39;)
        if not self.is_ttm or other.is_ttm:
            raise IncompatibleTypes(&#39;First operand should be a TT matrix and second a TT vector.&#39;)
            
        return dmrg_matvec(self, other, eps = eps, verb = verb, nswp = nswp)

    def apply_mask(self,indices):
        &#34;&#34;&#34;
        Evaluate the tensor on the given index list.

        Args:
            indices (list[list[int]]): the index list where the tensor should be evaluated. Length is M.

        Returns:
            torch.tensor: the values of the tensor

        Examples:
            x = torchtt.random([10,12,14],[1,4,5,1])
            indices = torch.tensor([[0,0,0],[1,2,3],[1,1,1]])
            val = x.apply_mask(indices)
        &#34;&#34;&#34;
        result = apply_mask(self.cores,self.R,indices)
        return result

    def __truediv__(self,other):
        &#34;&#34;&#34;
        This function implements the &#34;/&#34; operator.
        This operation is performed using the AMEN solver. The number of sweeps and rthe relative accuracy are fixed.
        For most cases it is sufficient but sometimes it can fail.
        Check the function torchtt.elementwise_divide() if you want to change the arguments of the AMEN solver.
        Example: z = 1.0/x # x is TT instance

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the first operand.

        Raises:
            IncompatibleTypes: Operands should be either TT or TTM.
            ShapeMismatch: Both operands should have the same shape.
            InvalidArguments: Operand not permitted. A TT-object can be divided only with scalars.
            
        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        if isinstance(other,int) or isinstance(other,float) or tn.is_tensor(other):
            # divide by a scalar
            cores_new = self.cores.copy()
            cores_new[0] /= other
            result = TT(cores_new)
        elif isinstance(other,TT):
            if self.is_ttm != other.is_ttm:
                raise IncompatibleTypes(&#39;Operands should be either TT or TTM.&#39;)
            if self.N != other.N or (self.is_ttm and self.M != other.M):
                raise ShapeMismatch(&#34;Both operands should have the same shape.&#34;)
            result = TT(amen_divide(other,self,50,None,1e-12,500,verbose=False))       
        else:
            raise InvalidArguments(&#39;Operand not permitted. A TT-object can be divided only with scalars.&#39;)
            
       
        return result
    
    def __rtruediv__(self,other):
        &#34;&#34;&#34;
        Right true division. this function is called when a non TT object is divided by a TT object.
        This operation is performed using the AMEN solver. The number of sweeps and rthe relative accuracy are fixed.
        For most cases it is sufficient but sometimes it can fail.
        Check the function torchtt.elementwise_divide() if you want to change the arguments of the AMEN solver.
        Example: z = 1.0/x # x is TT instance

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the first operand.

        Raises:
            InvalidArguments: The first operand must be int, float or 1d torch.tensor.
            
        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        if isinstance(other,int) or isinstance(other,float) or ( tn.is_tensor(other) and other.numel()==1):
            o = ones(self.N,dtype=self.cores[0].dtype,device = self.cores[0].device)
            o.cores[0] *= other
            cores_new = amen_divide(self,o,50,None,1e-12,500,verbose=False)
        else:
            raise InvalidArguments(&#34;The first operand must be int, float or 1d torch.tensor.&#34;)   
         
        return TT(cores_new)

    
    
    def t(self):
        &#34;&#34;&#34;
        Returns the transpoise of a given TT matrix.
        
        Raises:
            InvalidArguments: Has to TT matrix.
            
        Returns: 
            torchtt.TT: the transpose. 
        &#34;&#34;&#34; 
        if not self.is_ttm:
            raise InvalidArguments(&#39;Has to TT matrix.&#39;)
            
        cores_new = [tn.permute(c,[0,2,1,3]) for c in self.cores]
        
        return TT(cores_new)
        
    
    def norm(self,squared=False):
        &#34;&#34;&#34;
        Computes the frobenius norm of a TT object.

        Args:
            squared (bool, optional): returns the square of the norm if True. Defaults to False.

        Returns:
            torch.tensor: the norm.
        &#34;&#34;&#34;
        
        if any([c.requires_grad or c.grad_fn != None for c in self.cores]):
            norm = tn.tensor([[1.0]],dtype = self.cores[0].dtype, device=self.cores[0].device)
            
            if self.is_ttm:
                for i in range(len(self.N)):
                    norm = tn.einsum(&#39;ab,aijm,bijn-&gt;mn&#39;,norm, self.cores[i], self.cores[i])
                norm = tn.squeeze(norm)
            else:
                           
                for i in range(len(self.N)):
                    norm = tn.einsum(&#39;ab,aim,bin-&gt;mn&#39;,norm, self.cores[i], self.cores[i])
                norm = tn.squeeze(norm)
            if squared:
                return norm
            else:
                return tn.sqrt(tn.abs(norm))
 
        else:        
            d = len(self.cores)

            core_now = self.cores[0]
            for i in range(d-1):
                if self.is_ttm:
                    mode_shape = [core_now.shape[1],core_now.shape[2]]
                    core_now = tn.reshape(core_now,[core_now.shape[0]*core_now.shape[1]*core_now.shape[2],-1])
                else:
                    mode_shape = [core_now.shape[1]]
                    core_now = tn.reshape(core_now,[core_now.shape[0]*core_now.shape[1],-1])
                    
                # perform QR
                Qmat, Rmat = QR(core_now)
                     
                # take next core
                core_next = self.cores[i+1]
                shape_next = list(core_next.shape[1:])
                core_next = tn.reshape(core_next,[core_next.shape[0],-1])
                core_next = Rmat @ core_next
                core_next = tn.reshape(core_next,[Qmat.shape[1]]+shape_next)
                
                # update the cores
                
                core_now = core_next
            if squared:
                return tn.linalg.norm(core_next)**2
            else:
                return tn.linalg.norm(core_next)

    def sum(self,index = None):
        &#34;&#34;&#34;
        Contracts a tensor in the TT format along the given indices and retuyrns the resulting tensor in the TT format.
        If no index list is given, the sum over all indices is performed.

        Args:
            index (int or list[int] or None, optional): the indices along which the summation is performed. None selects all of them. Defaults to None.

        Raises:
            InvalidArguments: Invalid index.

        Returns:
            torchtt.TT or torch.tensor: the result.
        &#34;&#34;&#34;
        
        if index != None and isinstance(index,int):
            index = [index]
        if not isinstance(index,list) and index != None:
            raise InvalidArguments(&#39;Invalid index.&#39;)
             
        if index == None: 
            # the case we need to sum over all modes
            if self.is_ttm:
                C = tn.reduce_sum(self.cores[0],[0,1,2])
                for i in range(1,len(self.N)):
                    C = tn.sum(tn.einsum(&#39;i,ijkl-&gt;jkl&#39;,C,self.cores[i]),[0,1])
                S = tn.sum(C)
            else:
                C = tn.sum(self.cores[0],[0,1])
                for i in range(1,len(self.N)):
                    C = tn.sum(tn.einsum(&#39;i,ijk-&gt;jk&#39;,C,self.cores[i]),0)
                S = tn.sum(C)
        else:
            # we return the TT-tensor with summed indices
            cores = []
            
            if self.is_ttm:
                tmp = [1,2]
            else:
                tmp = [1]
                
            for i in range(len(self.N)):
                if i in index:
                    C = tn.sum(self.cores[i], tmp, keepdim = True)
                    cores.append(C)
                else:
                    cores.append(self.cores[i])
                        
            S = TT(cores)
            S.reduce_dims()
            
        return S

    def to_ttm(self):
        &#34;&#34;&#34;
        Converts a TT-tensor to the TT-matrix format. In the tensor has the shape N1 x ... x Nd, the result has the shape 
        N1 x ... x Nd x 1 x ... x 1.
    
        Returns:
            torch.TT: the result
        &#34;&#34;&#34;

        cores_new = [tn.reshape(c,(c.shape[0],c.shape[1],1,c.shape[2])) for c in self.cores]
        return TT(cores_new)

    def reduce_dims(self):
        &#34;&#34;&#34;
        Reduces the size 1 modes of the TT-object.
        At least one mode should be larger than 1.

        Returns:
            None.
        &#34;&#34;&#34;
        
        # TODO: implement a version that reduces the rank also. by spliting the cores with modes 1 into 2 using the SVD.
        
        if self.is_ttm:
            cores_new = []
            
            for i in range(len(self.N)):
                
                if self.cores[i].shape[1] == 1 and self.cores[i].shape[2] == 1:
                    if self.cores[i].shape[0] &gt; self.cores[i].shape[3] or i == len(self.N)-1:
                        # multiply to the left
                        if len(cores_new) &gt; 0:
                            cores_new[-1] = tn.einsum(&#39;ijok,kl-&gt;ijol&#39;,cores_new[-1], self.cores[i][:,0,0,:])
                        else: 
                            # there is no core to the left. Multiply right.
                            if i != len(self.N)-1:
                                self.cores[i+1] = tn.einsum(&#39;ij,jkml-&gt;ikml&#39;, self.cores[i][:,0,0,:],self.cores[i+1])
                            else:
                                cores_new.append(self.cores[i])
                            
                    else:
                        # multiply to the right. Set the carry 
                        self.cores[i+1] = tn.einsum(&#39;ij,jkml-&gt;ikml&#39;,self.cores[i][:,0,0,:],self.cores[i+1])
                        
                else:
                    cores_new.append(self.cores[i])
                    
            # update the cores and ranks and shape
            self.N = []
            self.M = []
            self.R = [1]
            for i in range(len(cores_new)):
                self.N.append(cores_new[i].shape[2])
                self.M.append(cores_new[i].shape[1])
                self.R.append(cores_new[i].shape[3])
            self.cores = cores_new
        else:
            cores_new = []
            
            for i in range(len(self.N)):
                
                if self.cores[i].shape[1] == 1:
                    if self.cores[i].shape[0] &gt; self.cores[i].shape[2] or i == len(self.N)-1:
                        # multiply to the left
                        if len(cores_new) &gt; 0:
                            cores_new[-1] = tn.einsum(&#39;ijk,kl-&gt;ijl&#39;,cores_new[-1], self.cores[i][:,0,:])
                        else: 
                            # there is no core to the left. Multiply right.
                            if i != len(self.N)-1:
                                self.cores[i+1] = tn.einsum(&#39;ij,jkl-&gt;ikl&#39;, self.cores[i][:,0,:],self.cores[i+1])
                            else:
                                cores_new.append(self.cores[i])
                                
                            
                    else:
                        # multiply to the right. Set the carry 
                        self.cores[i+1] = tn.einsum(&#39;ij,jkl-&gt;ikl&#39;,self.cores[i][:,0,:],self.cores[i+1])
                        
                else:
                    cores_new.append(self.cores[i])
            
            
            # update the cores and ranks and shape
            self.N = []
            self.R = [1]
            for i in range(len(cores_new)):
                self.N.append(cores_new[i].shape[1])
                self.R.append(cores_new[i].shape[2])
            self.cores = cores_new
                    
                    
        self.shape = [ (m,n) for m,n in zip(self.M,self.N) ] if self.is_ttm else [n for n in self.N] 
        
    def __getitem__(self,index):
        &#34;&#34;&#34;
        Performs slicinf of a TT object.
        Both TT matrix and TT tensor are supported.
        Similar to pytorch or numpy slicing.

        Args:
            index (tuple[slice] or tuple[int] or int or Ellipsis or slice): the slicing.

        Raises:
            NotImplementedError: Ellipsis are not supported.
            InvalidArguments: Slice size is invalid.
            InvalidArguments: Invalid slice. Tensor is not 1d.


        Returns:
            torchtt.TT or torch.tensor: the result. If all the indices are fixed, a scalar torch.tensor is returned otherwise a torchtt.TT.
        &#34;&#34;&#34;
        
        
        # slicing function
        
        ##### TODO: include Ellipsis support.
        
        # if a slice containg integers is passed, an element is returned
        # if ranged slices are used, a TT-object has to be returned.
        
        if isinstance(index,tuple):
            # check if more than one Ellipsis are to be found.
            if index.count(Ellipsis) &gt; 0:
                raise NotImplementedError(&#39;Ellipsis are not supported.&#39;)
            if self.is_ttm:
                if len(index) != len(self.N)*2:
                    raise InvalidArguments(&#39;Slice size is invalid.&#39;)
                    
                cores_new = []
                for i in range(len(self.cores)):
                    # cores_new.append(self.cores[i][:,index[i],index[i+len(self.N)],:])
                    if isinstance(index[i],slice):
                        cores_new.append(self.cores[i][:,index[i],index[i+len(self.N)],:])
                    else:
                        cores_new.append(tn.reshape(self.cores[i][:,index[i],index[i+len(self.N)],:],[self.R[i],1,1,self.R[i+1]]))
               
                
            else:
                if len(index) != len(self.N):
                    raise InvalidArguments(&#39;Slice size is invalid.&#39;)
                    
                cores_new = []
                for i in range(len(self.cores)):
                    if isinstance(index[i],slice):
                        cores_new.append(self.cores[i][:,index[i],:])
                    else:
                        cores_new.append(tn.reshape(self.cores[i][:,index[i],:],[self.R[i],-1,self.R[i+1]]))
            
            sliced = TT(cores_new)
            sliced.reduce_dims()
            if (sliced.is_ttm == False and sliced.N == [1]) or (sliced.is_ttm and sliced.N == [1] and sliced.M == [1]):
                sliced = tn.squeeze(sliced.cores[0])
                
                
            # cores = None
            
            
        elif isinstance(index,int):
            # tensor is 1d and one element is retrived
            if len(self.N) == 1:
                sliced = self.cores[0][0,index,0]
            else:
                raise InvalidArguments(&#39;Invalid slice. Tensor is not 1d.&#39;)
                
            ## TODO
        elif isinstance(index,Ellipsis):
            # return a copy of the tensor
            sliced = TT([c.clone() for c in self.cores])
            
        elif isinstance(index,slice):
            # tensor is 1d and one slice is extracted
            if len(self.N) == 1:
                sliced = TT(self.cores[0][:,index,:])
            else:
                raise InvalidArguments(&#39;Invalid slice. Tensor is not 1d.&#39;)
            ## TODO
        else:
            raise InvalidArguments(&#39;Invalid slice.&#39;)
            
        
        return sliced
    
    def __pow__(self,other):
        &#34;&#34;&#34;
        Computes the tensor Kronecker product.
        This implements the &#34;**&#34; operator.
        If None is provided as input the reult is the other tensor.
        If A is N_1 x ... x N_d and B is M_1 x ... x M_p, then kron(A,B) is N_1 x ... x N_d x M_1 x ... x M_p


        Args:
            first (torchtt.TT or None): first argument.
            second (torchtt.TT or none): second argument.

        Raises:
            IncompatibleTypes: Incompatible data types (make sure both are either TT-matrices or TT-tensors).
            InvalidArguments: Invalid arguments.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        result = kron(self,other)
        
        return result
    
    def __rpow__(self,other):
        &#34;&#34;&#34;
        Computes the tensor Kronecker product.
        This implements the &#34;**&#34; operator.
        If None is provided as input the reult is the other tensor.
        If A is N_1 x ... x N_d and B is M_1 x ... x M_p, then kron(A,B) is N_1 x ... x N_d x M_1 x ... x M_p


        Args:
            first (torchtt.TT or None): first argument.
            second (torchtt.TT or none): second argument.

        Raises:
            IncompatibleTypes: Incompatible data types (make sure both are either TT-matrices or TT-tensors).
            InvalidArguments: Invalid arguments.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        result = kron(self,other)
        
        return result
    
    def __neg__(self):
        &#34;&#34;&#34;
        Returns the negative of a given TT tensor.
        This implements the unery operator &#34;-&#34;

        Returns:
            torchtt.TT: the negated tensor.
        &#34;&#34;&#34;
    
        cores_new = [c.clone() for c in self.cores]
        cores_new[0] = -cores_new[0]
        return TT(cores_new)
    
    def __pos__(self):
        &#34;&#34;&#34;
        Implements the unary &#34;+&#34; operator returning a copy o the tensor.

        Returns:
            torchtt.TT: the tensor clone.
        &#34;&#34;&#34;
        
        cores_new = [c.clone() for c in self.cores]

        return TT(cores_new)
    
    def round(self, eps=1e-12, rmax = 2048): 
        &#34;&#34;&#34;
        Implements the rounding operations within a given tolerance epsilon.
        The maximum rank is also provided.

        Args:
            eps (float, optional): the relative accuracy. Defaults to 1e-12.
            rmax (int, optional): the maximum rank. Defaults to 2048.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        # rmax is not list
        if not isinstance(rmax,list):
            rmax = [1] + len(self.N)*[rmax] + [1]
            
        # call the round function
        tt_cores, R = round_tt(self.cores, self.R.copy(), eps, rmax,self.is_ttm)
        # creates a new TT and return it
        T = TT(tt_cores)
               
        return T
    
    def to_qtt(self, eps = 1e-12, mode_size = 2, rmax = 2048):
        &#34;&#34;&#34;
        Converts a tensor to the QTT format: N1 x N2 x ... x Nd -&gt; mode_size x mode_size x ... x mode_size.
        The product of the mode sizes should be a power of mode_size.
        The tensor in QTT can be converted back using the qtt_to_tens() method.

        Args:
            eps (flaot,optional): the accuracy. Defaults to 1e-12.
            mode_size (int, optional): the size of the modes. Defaults to 2.
            rmax (int): the maximum rank. Defaults to 2048.
            

        Raises:
            ShapeMismatch: Only quadratic TTM can be tranformed to QTT.
            ShapeMismatch: Reshaping error: check if the dimensions are powers of the desired mode size.

        Returns:
            torchtt.TT: the resulting reshaped tensor.
            
        Examples:
            import torchtt
            x = torchtt.random([16,8,64,128],[1,2,10,12,1])
            x_qtt = x.to_qtt()
            print(x_qtt)
            xf = x_qtt.qtt_to_tens(x.N) # a TT-rounding is recommended.
            
        &#34;&#34;&#34;
       
        cores_new = []
        if self.is_ttm:
            shape_new = []
            for i in range(len(self.N)):
                if self.N[i]!=self.M[i]:
                    raise ShapeMismatch(&#39;Only quadratic TTM can be tranformed to QTT.&#39;)
                if self.N[i]==mode_size**int(math.log(self.N[i],mode_size)):
                    shape_new += [(mode_size,mode_size)]*int(math.log(self.N[i],mode_size))
                else:
                    raise ShapeMismatch(&#39;Reshaping error: check if the dimensions are powers of the desired mode size:\r\ncore size &#39;+str(list(self.cores[i].shape))+&#39; cannot be reshaped.&#39;)
                
            result = reshape(self, shape_new, eps, rmax)
        else:
            for core in self.cores:
                if int(math.log(core.shape[1],mode_size))&gt;2:
                    Nnew = [core.shape[0]*mode_size]+[mode_size]*(int(math.log(core.shape[1],mode_size))-2)+[core.shape[2]*mode_size]
                    try:
                        core = tn.reshape(core,Nnew)
                    except:
                        raise ShapeMismatch(&#39;Reshaping error: check if the dimensions care powers of the desired mode size:\r\ncore size &#39;+str(list(core.shape))+&#39; cannot be reshaped to &#39;+str(Nnew))
                    cores,_ = to_tt(core,Nnew,eps,rmax,is_sparse=False)
                    cores_new.append(tn.reshape(cores[0],[-1,mode_size,cores[0].shape[-1]]))
                    cores_new += cores[1:-1]
                    cores_new.append(tn.reshape(cores[-1],[cores[-1].shape[0],mode_size,-1]))
                else: 
                    cores_new.append(core)
            result = TT(cores_new)
            
        return result
               
    def qtt_to_tens(self, original_shape):
        &#34;&#34;&#34;
        Transform a tensor back from QTT.

        Args:
            original_shape (list): the original shape.

        Raises:
            InvalidArguments: Original shape must be a list.
            ShapeMismatch: Mode sizes do not match.

        Returns:
            torchtt.TT: the folded tensor.
        &#34;&#34;&#34;
        
        if not isinstance(original_shape,list):
            raise InvalidArguments(&#34;Original shape must be a list.&#34;)

        core = None
        cores_new = []
        
        if self.is_ttm:
            pass
        else:
            k = 0
            for c in self.cores:
                if core==None:
                    core = c
                    so_far = core.shape[1]
                else:
                    core = tn.einsum(&#39;...i,ijk-&gt;...jk&#39;,core,c)
                    so_far *= c.shape[1]
                if so_far==original_shape[k]:
                    core = tn.reshape(core,[core.shape[0],-1,core.shape[-1]])
                    cores_new.append(core)
                    core = None
                    k += 1
            if k!= len(original_shape):
                raise ShapeMismatch(&#39;Mode sizes do not match.&#39;)
        return TT(cores_new)
    
    def mprod(self, factor_matrices, mode):
        &#34;&#34;&#34;
        n-mode product.

        Args:
            factor_matrices (torch.tensor or list[torch.tensor]): either a single matrix is directly provided or a list of matrices for product along multiple modes.
            mode (int or list[int]): the mode for the product. If factor_matrices is a torch.tensor then mode is an integer and the multiplication will be performed along a single mode.
                                     If factor_matrices is a list, the mode has to be list[int] of equal size.

        Raises:
            InvalidArguments: Invalid arguments.

        Returns:
            torchtt.TT: the result
        &#34;&#34;&#34;
    
        if isinstance(factor_matrices,list) and isinstance(mode, list):
            cores_new = [c.clone() for c in self.cores]
            for i in range(len(factor_matrices)):
                cores_new[mode[i]] =  tn.einsum(&#39;imjk,lj-&gt;imlk&#39;,cores_new[mode[i]],factor_matrices[i]) if self.is_ttm else tn.einsum(&#39;ijk,lj-&gt;ilk&#39;,cores_new[mode[i]],factor_matrices[i]) 
        elif isinstance(factor_matrices, tn.tensor) and isinstance(mode, int):
            cores_new = [c.clone() for c in self.cores]
            cores_new[mode] =  tn.einsum(&#39;imjk,lj-&gt;imlk&#39;,cores_new[mode],factor_matrices) if self.is_ttm else tn.einsum(&#39;ijk,lj-&gt;ilk&#39;,cores_new[mode],factor_matrices) 
        else:
            raise InvalidArguments(&#39;Invalid arguments.&#39;)
        
        return TT(cores_new)        
        
    
def eye(shape, dtype=tn.float64, device = None):
    &#34;&#34;&#34;
    Construct the TT decomposition of a multidimensional identity matrix.
    all the TT ranks are 1.

    Args:
        shape (list[int]): the shape.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Returns:
        torchtt.TT: the one tensor.
    &#34;&#34;&#34;
    
    shape = list(shape)
    
    cores = [tn.unsqueeze(tn.unsqueeze(tn.eye(s, dtype=dtype, device = device),0),3) for s in shape]            
    
    return TT(cores)
    
def zeros(shape, dtype=tn.float64, device = None):
    &#34;&#34;&#34;
    Construct a tensor that contains only ones.
    the shape can be a list of ints or a list of tuples of ints. The second case creates a TT matrix.

    Args:
        shape (list[int] or list[tuple[int]]): the shape.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Raises:
        InvalidArguments: Shape must be a list.

    Returns:
        torchtt.TT: the one tensor.
    &#34;&#34;&#34;
    if isinstance(shape,list):
        d = len(shape)
        if isinstance(shape[0],tuple):
            # we create a TT-matrix
            cores = [tn.zeros([1,shape[i][0],shape[i][1],1],dtype=dtype, device = device) for i in range(d)]            
            
        else:
            # we create a TT-tensor
            cores = [tn.zeros([1,shape[i],1],dtype=dtype, device = device) for i in range(d)]
            
    else:
        raise InvalidArguments(&#39;Shape must be a list.&#39;)
    
    return TT(cores)
    

def emptyTT():
    
    tens = TT(None)
    tens.is_ttm = False
    tens.N = []
    tens.R = []
    return tens
    
def emptyTTM():
    
    tens = TT(None)
    tens.is_ttm = True
    tens.N = []
    tens.R = []
    return tens
  
def kron(first, second):
    &#34;&#34;&#34;
    Computes the tensor Kronecker product.
    If None is provided as input the reult is the other tensor.
    If A is N_1 x ... x N_d and B is M_1 x ... x M_p, then kron(A,B) is N_1 x ... x N_d x M_1 x ... x M_p


    Args:
        first (torchtt.TT or None): first argument.
        second (torchtt.TT or none): second argument.

    Raises:
        IncompatibleTypes: Incompatible data types (make sure both are either TT-matrices or TT-tensors).
        InvalidArguments: Invalid arguments.

    Returns:
        torchtt.TT: the result.
    &#34;&#34;&#34;
    if first == None and isinstance(second,TT):
        cores_new = [c.clone() for c in second.cores]
        result = TT(cores_new)
    elif second == None and isinstance(first,TT): 
        cores_new = [c.clone() for c in first.cores]
        result = TT(cores_new)
    elif isinstance(first,TT) and isinstance(second,TT):
        if first.is_ttm != second.is_ttm:
            raise IncompatibleTypes(&#39;Incompatible data types (make sure both are either TT-matrices or TT-tensors).&#39;)
    
        # concatenate the result
        cores_new = [c.clone() for c in first.cores] + [c.clone() for c in second.cores]
        result = TT(cores_new)
    else:
        raise InvalidArguments(&#39;Invalid arguments.&#39;)
    return result



def ones(shape, dtype=tn.float64, device = None):
    &#34;&#34;&#34;
    Construct a tensor that contains only ones.
    the shape can be a list of ints or a list of tuples of ints. The second case creates a TT matrix.

    Args:
        shape (list[int] or list[tuple[int]]): the shape.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Raises:
        InvalidArguments: Shape must be a list.

    Returns:
        torchtt.TT: the one tensor.
    &#34;&#34;&#34;
    if isinstance(shape,list):
        d = len(shape)
        if d==0:
            return TT(None)
        else:
            if isinstance(shape[0],tuple):
                # we create a TT-matrix
                cores = [tn.ones([1,shape[i][0],shape[i][1],1],dtype=dtype,device=device) for i in range(d)]            
                
            else:
                # we create a TT-tensor
                cores = [tn.ones([1,shape[i],1],dtype=dtype,device=device) for i in range(d)]
            
    else:
        raise InvalidArguments(&#39;Shape must be a list.&#39;)
    
    return TT(cores)


def random(N, R, dtype = tn.float64, device = None):
    &#34;&#34;&#34;
    Returns a tensor of shape N with random cores of rank R.
    Each core is a normal distributed with mean 0 and variance 1.
    Check also the method torchtt.randn()for better random tensors in the TT format.

    Args:
        N (list[int] or list[tuple[int]]): the shape of the tensor. If the elements are tuples of integers, we deal with a TT-matrix.
        R (list[int] or int): can be a list if the exact rank is specified or an integer if the maximum rank is secified.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Raises:
        InvalidArguments: Check if N and R are right.

    Returns:
        torchtt.TT: the result.
    &#34;&#34;&#34;
    
    if isinstance(R,int):
        R = [1]+[R]*(len(N)-1)+[1]
    elif len(N)+1 != len(R) or R[0] != 1 or R[-1] != 1 or len(N)==0:
        raise InvalidArguments(&#39;Check if N and R are right.&#39;)
        
    cores = []
    
    for i in range(len(N)):
        cores.append(tn.randn([R[i],N[i][0],N[i][1],R[i+1]] if isinstance(N[i],tuple) else [R[i],N[i],R[i+1]], dtype = dtype, device = device))
        
    T = TT(cores)
    
    return T

def randn(N, R, var = 1.0, dtype = tn.float64, device = None):
    &#34;&#34;&#34;
    A torchtt.TT tensor of shape N = [N1 x ... x Nd] and rank R is returned. 
    The entries of the fuill tensor are alomst normal distributed with the variance var.
    
    Args:
        N (list[int]): the shape.
        R (list[int]): the rank.
        var (float, optional): the variance. Defaults to 1.0.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Returns:
        torchtt.TT: the result.
    &#34;&#34;&#34;

    d = len(N)
    v1 = var / np.prod(R)
    v = v1**(1/d)
    cores = [None] * d
    for i in range(d):
        cores[i] = tn.randn([R[i],N[i][0],N[i][1],R[i+1]] if isinstance(N[i],tuple) else [R[i],N[i],R[i+1]], dtype = dtype, device = device)*np.sqrt(v)

    return TT(cores)

def reshape(tens, shape, eps = 1e-16, rmax = 2048):
    &#34;&#34;&#34;
    Reshapes a torchtt.TT tensor in the TT format.
    A rounding is also performed.
    
    Args:
        tens (torchtt.TT): the input tensor.
        shape (list[int] or list[tuple[int]]): the desired shape. In the case of a TT operator the shape has to be given as list of tuples of ints [(M1,N1),...,(Md,Nd)].
        eps (float, optional): relative accuracy. Defaults to 1e-16.
        rmax (int, optional): maximum rank. Defaults to 2048.

    Raises:
        ShapeMismatch: Tthe product of modes should remain equal. Check the given shape.

    Returns:
        torchtt.TT: the resulting tensor.
    &#34;&#34;&#34;
    
    
    if tens.is_ttm:
        M = []
        N = []
        for t in shape:
            M.append(t[0])
            N.append(t[1])
        if np.prod(tens.N)!=np.prod(N) or np.prod(tens.M)!=np.prod(M):
            raise ShapeMismatch(&#39;Tthe product of modes should remain equal. Check the given shape.&#39;)
        core = tens.cores[0]
        cores_new = []
        
        idx = 0
        idx_shape = 0
        
        while True:
            if core.shape[1] % M[idx_shape] == 0 and core.shape[2] % N[idx_shape] == 0:
                if core.shape[1] // M[idx_shape] &gt; 1 and core.shape[2] // N[idx_shape] &gt; 1:
                    m1 = M[idx_shape]
                    m2 = core.shape[1] // m1
                    n1 = N[idx_shape]
                    n2 = core.shape[2] // n1
                    r1 = core.shape[0]
                    r2 = core.shape[-1]
                    tmp = tn.reshape(core,[r1*m1,m2,n1,n2*r2])
                    
                    crz,_ = mat_to_tt(tmp, [r1*m1,m2], [n1,n2*r2], eps, rmax)
                    
                    cores_new.append(tn.reshape(crz[0],[r1,m1,n1,-1]))
                    
                    core = tn.reshape(crz[1],[-1,m2,n2,r2]) 
                else:
                    cores_new.append(core+0)
                    if idx == len(tens.cores)-1:
                        break
                    else:
                        idx+=1
                        core = tens.cores[idx]
                idx_shape += 1
                if idx_shape == len(shape):
                    break
            else: 
                idx += 1
                if idx&gt;=len(tens.cores):
                    break
                
                core = tn.einsum(&#39;ijkl,lmno-&gt;ijmkno&#39;,core,tens.cores[idx])
                core = tn.reshape(core,[core.shape[0],core.shape[1]*core.shape[2],-1,core.shape[-1]])
                
    else:
        if np.prod(tens.N)!=np.prod(shape):
            raise ShapeMismatch(&#39;Tthe product of modes should remain equal. Check the given shape.&#39;)
            
        core = tens.cores[0]
        cores_new = []
        
        idx = 0
        idx_shape = 0
        while True:
            if core.shape[1] % shape[idx_shape] == 0:
                if core.shape[1] // shape[idx_shape] &gt; 1:
                    s1 = shape[idx_shape]
                    s2 = core.shape[1] // s1
                    r1 = core.shape[0]
                    r2 = core.shape[2]
                    tmp = tn.reshape(core,[r1*s1,s2*r2])
                    
                    crz,_ = to_tt(tmp,tmp.shape,eps,rmax)
                    
                    cores_new.append(tn.reshape(crz[0],[r1,s1,-1]))
                    
                    core = tn.reshape(crz[1],[-1,s2,r2]) 
                else:
                    cores_new.append(core+0)
                    if idx == len(tens.cores)-1:
                        break
                    else:
                        idx+=1
                        core = tens.cores[idx]
                idx_shape += 1
                if idx_shape == len(shape):
                    break
            else: 
                idx += 1
                if idx&gt;=len(tens.cores):
                    break
                
                core = tn.einsum(&#39;ijk,klm-&gt;ijlm&#39;,core,tens.cores[idx])
                core = tn.reshape(core,[core.shape[0],-1,core.shape[-1]])
                
    return TT(cores_new).round(eps)
        
        
def meshgrid(vectors):
    &#34;&#34;&#34;
    Creates a meshgrid of torchtt.TT objects. Similar to numpy.meshgrid or torch.meshgrid.
    The input is a list of d torch.tensor vectors of sizes N_1, ... ,N_d
    The result is a list of torchtt.TT instances of shapes N1 x ... x Nd.
    
    Args:
        vectors (list[torch.tensor]): the vectors (1d tensors).

    Returns:
        list[torchtt.TT]: the resulting meshgrid.
    &#34;&#34;&#34;
    
    Xs = []
    dtype = vectors[0].dtype
    for i in range(len(vectors)):
        lst = [tn.ones((1,v.shape[0],1),dtype=dtype) for v in vectors]
        lst[i] = tn.reshape(vectors[i],[1,-1,1])
        Xs.append(TT(lst))
    return Xs
    
def dot(a,b,axis=None):
    &#34;&#34;&#34;
    Computes the dot product between 2 tensors in TT format.
    If both a and b have identical mode sizes the result is the dot product.
    If a and b have inequal mode sizes, the function perform index contraction. 
    The number of dimensions of a must be greater or equal as b.
    The modes of the tensor a along which the index contraction with b is performed are given in axis.

    Args:
        a (torchtt.TT): the first tensor.
        b (torchtt.TT): the second tensor.
        axis (list[int], optional): the mode indices for index contraction. Defaults to None.

    Raises:
        InvalidArguments: Both operands should be TT instances.
        NotImplementedError: Operation not implemented for TT-matrices.
        ShapeMismatch: Operands are not the same size.
        ShapeMismatch: Number of the modes of the first tensor must be equal with the second.

    Returns:
        float or torchtt.TT: the result. If no axis index is provided the result is a scalar otherwise a torchtt.TT object.
    &#34;&#34;&#34;
    
    if not isinstance(a, TT) or not isinstance(b, TT):
        raise InvalidArguments(&#39;Both operands should be TT instances.&#39;)
    
    
    if axis == None:
        # treat first the full dot product
        # faster than partial projection
        if a.is_ttm or b.is_ttm:
            raise NotImplementedError(&#39;Operation not implemented for TT-matrices.&#39;)
        if a.N != b.N:
            raise ShapeMismatch(&#39;Operands are not the same size.&#39;)
        
        result = tn.tensor([[1.0]],dtype = a.cores[0].dtype, device=a.cores[0].device)
        
        for i in range(len(a.N)):
            result = tn.einsum(&#39;ab,aim,bin-&gt;mn&#39;,result, a.cores[i], b.cores[i])
        result = tn.squeeze(result)
    else:
        # partial case
        if a.is_ttm or b.is_ttm:
            raise NotImplementedError(&#39;Operation not implemented for TT-matrices.&#39;)
        if len(a.N)&lt;len(b.N):
            raise ShapeMismatch(&#39;Number of the modes of the first tensor must be equal with the second.&#39;)
        # if a.N[axis] != b.N:
        #     raise Exception(&#39;Dimension mismatch.&#39;)
        
        k = 0 # index for the tensor b
        cores_new = []
        rank_left = 1
        for i in range(len(a.N)):
            if i in axis:
                cores_new.append(b.cores[k])
                rank_left = b.cores[k].shape[2]
                k+=1
            else:
                rank_right = b.cores[k].shape[0] if i+1 in axis else rank_left                
                cores_new.append(tn.einsum(&#39;ik,j-&gt;ijk&#39;,tn.eye(rank_left,rank_right,dtype=a.cores[0].dtype),tn.ones([a.N[i]],dtype=a.cores[0].dtype)))
        
        result = (a*TT(cores_new)).sum(axis)
    return result

def bilinear_form(x,A,y):
    &#34;&#34;&#34;
    Computes the bilinear form x^T A y for TT tensors:

    Args:
        x (torchtt.TT): the tensors.
        A (torchtt.TT): the tensors (must be TT matrix).
        y (torchtt.TT): the tensors.

    Raises:
        InvalidArguments: Inputs must be torchtt.TT instances.
        IncompatibleTypes: x and y must be TT tensors and A must be TT matrix.
        ShapeMismatch: Check the shapes. Required is x.N == A.M and y.N == A.N.

    Returns:
        torch.tensor: the result of the bilienar form as tensor with 1 element.
    &#34;&#34;&#34;
    if not isinstance(x,TT) or not isinstance(A,TT) or not isinstance(y,TT):
        raise InvalidArguments(&#34;Inputs must be torchtt.TT instances.&#34;)
    if x.is_ttm or y.is_ttm or A.is_ttm==False:
        raise IncompatibleTypes(&#34;x and y must be TT tensors and A must be TT matrix.&#34;)
    if x.N != A.M or y.N != A.N:
        raise ShapeMismatch(&#34;Check the shapes. Required is x.N == A.M and y.N == A.N.&#34;)
    d = len(x.N)
    return bilinear_form_aux(x.cores,A.cores,y.cores,d)

def elementwise_divide(x, y, eps = 1e-12, starting_tensor = None, nswp = 50, kick = 4, verbose = False):
    &#34;&#34;&#34;
    Perform the elemntwise division x/y of two tensors in the TT format using the AMEN method.
    Use this method if different AMEN arguments are needed.
    This method does not check the validity of the inputs.
    
    Args:
        x (torchtt.TT or scalar): first tensor (can also be scalar of type float, int, torch.tensor with shape (1)).
        y (torchtt.TT): second tensor.
        eps (float, optional): relative acccuracy. Defaults to 1e-12.
        starting_tensor (torchtt.tensor or None, optional): initial guess of the result (None for random initial guess). Defaults to None.
        nswp (int, optional): number of iterations. Defaults to 50.
        kick (int, optional): size of rank enrichment. Defaults to 4.
        verbose (bool, optional): display debug info. Defaults to False.

    Returns:
        torchtt.TT: the result
    &#34;&#34;&#34;

    cores_new = amen_divide(y,x,nswp,starting_tensor,eps,rmax = 1000, kickrank = kick, verbose=verbose)
    return TT(cores_new)

def rank1TT(vectors):
    &#34;&#34;&#34;
    Compute the rank 1 TT from a list of vectors.

    Args:
        vectors (list[torch.tensor]): the list of vectors.

    Returns:
        torchtt.TT: the resulting TT object.
    &#34;&#34;&#34;
    
    return TT([tn.reshape(vectors[i],[1,-1,1]) for i in range(len(vectors))])

 
def numel(tensor):
    &#34;&#34;&#34;
    Return the number of entries needed to store the TT cores for the given tensor.

    Args:
        tensor (torchtt.TT): the TT representation of the tensor.

    Returns:
        int: number of floats stored for the TT decomposition.
    &#34;&#34;&#34;
    
    return sum([tn.numel(tensor.cores[i]) for i in range(len(tensor.N))])</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="torchtt.torchtt.bilinear_form"><code class="name flex">
<span>def <span class="ident">bilinear_form</span></span>(<span>x, A, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the bilinear form x^T A y for TT tensors:</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torchtt.TT</code></dt>
<dd>the tensors.</dd>
<dt><strong><code>A</code></strong> :&ensp;<code>torchtt.TT</code></dt>
<dd>the tensors (must be TT matrix).</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torchtt.TT</code></dt>
<dd>the tensors.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Inputs must be torchtt.TT instances.</dd>
<dt><code>IncompatibleTypes</code></dt>
<dd>x and y must be TT tensors and A must be TT matrix.</dd>
<dt><code>ShapeMismatch</code></dt>
<dd>Check the shapes. Required is x.N == A.M and y.N == A.N.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>the result of the bilienar form as tensor with 1 element.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bilinear_form(x,A,y):
    &#34;&#34;&#34;
    Computes the bilinear form x^T A y for TT tensors:

    Args:
        x (torchtt.TT): the tensors.
        A (torchtt.TT): the tensors (must be TT matrix).
        y (torchtt.TT): the tensors.

    Raises:
        InvalidArguments: Inputs must be torchtt.TT instances.
        IncompatibleTypes: x and y must be TT tensors and A must be TT matrix.
        ShapeMismatch: Check the shapes. Required is x.N == A.M and y.N == A.N.

    Returns:
        torch.tensor: the result of the bilienar form as tensor with 1 element.
    &#34;&#34;&#34;
    if not isinstance(x,TT) or not isinstance(A,TT) or not isinstance(y,TT):
        raise InvalidArguments(&#34;Inputs must be torchtt.TT instances.&#34;)
    if x.is_ttm or y.is_ttm or A.is_ttm==False:
        raise IncompatibleTypes(&#34;x and y must be TT tensors and A must be TT matrix.&#34;)
    if x.N != A.M or y.N != A.N:
        raise ShapeMismatch(&#34;Check the shapes. Required is x.N == A.M and y.N == A.N.&#34;)
    d = len(x.N)
    return bilinear_form_aux(x.cores,A.cores,y.cores,d)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.dot"><code class="name flex">
<span>def <span class="ident">dot</span></span>(<span>a, b, axis=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the dot product between 2 tensors in TT format.
If both a and b have identical mode sizes the result is the dot product.
If a and b have inequal mode sizes, the function perform index contraction.
The number of dimensions of a must be greater or equal as b.
The modes of the tensor a along which the index contraction with b is performed are given in axis.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>torchtt.TT</code></dt>
<dd>the first tensor.</dd>
<dt><strong><code>b</code></strong> :&ensp;<code>torchtt.TT</code></dt>
<dd>the second tensor.</dd>
<dt><strong><code>axis</code></strong> :&ensp;<code>list[int]</code>, optional</dt>
<dd>the mode indices for index contraction. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Both operands should be TT instances.</dd>
<dt><code>NotImplementedError</code></dt>
<dd>Operation not implemented for TT-matrices.</dd>
<dt><code>ShapeMismatch</code></dt>
<dd>Operands are not the same size.</dd>
<dt><code>ShapeMismatch</code></dt>
<dd>Number of the modes of the first tensor must be equal with the second.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code> or <code>torchtt.TT</code></dt>
<dd>the result. If no axis index is provided the result is a scalar otherwise a torchtt.TT object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dot(a,b,axis=None):
    &#34;&#34;&#34;
    Computes the dot product between 2 tensors in TT format.
    If both a and b have identical mode sizes the result is the dot product.
    If a and b have inequal mode sizes, the function perform index contraction. 
    The number of dimensions of a must be greater or equal as b.
    The modes of the tensor a along which the index contraction with b is performed are given in axis.

    Args:
        a (torchtt.TT): the first tensor.
        b (torchtt.TT): the second tensor.
        axis (list[int], optional): the mode indices for index contraction. Defaults to None.

    Raises:
        InvalidArguments: Both operands should be TT instances.
        NotImplementedError: Operation not implemented for TT-matrices.
        ShapeMismatch: Operands are not the same size.
        ShapeMismatch: Number of the modes of the first tensor must be equal with the second.

    Returns:
        float or torchtt.TT: the result. If no axis index is provided the result is a scalar otherwise a torchtt.TT object.
    &#34;&#34;&#34;
    
    if not isinstance(a, TT) or not isinstance(b, TT):
        raise InvalidArguments(&#39;Both operands should be TT instances.&#39;)
    
    
    if axis == None:
        # treat first the full dot product
        # faster than partial projection
        if a.is_ttm or b.is_ttm:
            raise NotImplementedError(&#39;Operation not implemented for TT-matrices.&#39;)
        if a.N != b.N:
            raise ShapeMismatch(&#39;Operands are not the same size.&#39;)
        
        result = tn.tensor([[1.0]],dtype = a.cores[0].dtype, device=a.cores[0].device)
        
        for i in range(len(a.N)):
            result = tn.einsum(&#39;ab,aim,bin-&gt;mn&#39;,result, a.cores[i], b.cores[i])
        result = tn.squeeze(result)
    else:
        # partial case
        if a.is_ttm or b.is_ttm:
            raise NotImplementedError(&#39;Operation not implemented for TT-matrices.&#39;)
        if len(a.N)&lt;len(b.N):
            raise ShapeMismatch(&#39;Number of the modes of the first tensor must be equal with the second.&#39;)
        # if a.N[axis] != b.N:
        #     raise Exception(&#39;Dimension mismatch.&#39;)
        
        k = 0 # index for the tensor b
        cores_new = []
        rank_left = 1
        for i in range(len(a.N)):
            if i in axis:
                cores_new.append(b.cores[k])
                rank_left = b.cores[k].shape[2]
                k+=1
            else:
                rank_right = b.cores[k].shape[0] if i+1 in axis else rank_left                
                cores_new.append(tn.einsum(&#39;ik,j-&gt;ijk&#39;,tn.eye(rank_left,rank_right,dtype=a.cores[0].dtype),tn.ones([a.N[i]],dtype=a.cores[0].dtype)))
        
        result = (a*TT(cores_new)).sum(axis)
    return result</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.elementwise_divide"><code class="name flex">
<span>def <span class="ident">elementwise_divide</span></span>(<span>x, y, eps=1e-12, starting_tensor=None, nswp=50, kick=4, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform the elemntwise division x/y of two tensors in the TT format using the AMEN method.
Use this method if different AMEN arguments are needed.
This method does not check the validity of the inputs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torchtt.TT</code> or <code>scalar</code></dt>
<dd>first tensor (can also be scalar of type float, int, torch.tensor with shape (1)).</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torchtt.TT</code></dt>
<dd>second tensor.</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>relative acccuracy. Defaults to 1e-12.</dd>
<dt><strong><code>starting_tensor</code></strong> :&ensp;<code>torchtt.tensor</code> or <code>None</code>, optional</dt>
<dd>initial guess of the result (None for random initial guess). Defaults to None.</dd>
<dt><strong><code>nswp</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of iterations. Defaults to 50.</dd>
<dt><strong><code>kick</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>size of rank enrichment. Defaults to 4.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>display debug info. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the result</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def elementwise_divide(x, y, eps = 1e-12, starting_tensor = None, nswp = 50, kick = 4, verbose = False):
    &#34;&#34;&#34;
    Perform the elemntwise division x/y of two tensors in the TT format using the AMEN method.
    Use this method if different AMEN arguments are needed.
    This method does not check the validity of the inputs.
    
    Args:
        x (torchtt.TT or scalar): first tensor (can also be scalar of type float, int, torch.tensor with shape (1)).
        y (torchtt.TT): second tensor.
        eps (float, optional): relative acccuracy. Defaults to 1e-12.
        starting_tensor (torchtt.tensor or None, optional): initial guess of the result (None for random initial guess). Defaults to None.
        nswp (int, optional): number of iterations. Defaults to 50.
        kick (int, optional): size of rank enrichment. Defaults to 4.
        verbose (bool, optional): display debug info. Defaults to False.

    Returns:
        torchtt.TT: the result
    &#34;&#34;&#34;

    cores_new = amen_divide(y,x,nswp,starting_tensor,eps,rmax = 1000, kickrank = kick, verbose=verbose)
    return TT(cores_new)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.emptyTT"><code class="name flex">
<span>def <span class="ident">emptyTT</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def emptyTT():
    
    tens = TT(None)
    tens.is_ttm = False
    tens.N = []
    tens.R = []
    return tens</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.emptyTTM"><code class="name flex">
<span>def <span class="ident">emptyTTM</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def emptyTTM():
    
    tens = TT(None)
    tens.is_ttm = True
    tens.N = []
    tens.R = []
    return tens</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.eye"><code class="name flex">
<span>def <span class="ident">eye</span></span>(<span>shape, dtype=torch.float64, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Construct the TT decomposition of a multidimensional identity matrix.
all the TT ranks are 1.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>the shape.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>torch.dtype</code>, optional</dt>
<dd>the dtype of the returned tensor. Defaults to tn.float64.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code>, optional</dt>
<dd>the device where the TT cores are created (None means CPU). Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the one tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eye(shape, dtype=tn.float64, device = None):
    &#34;&#34;&#34;
    Construct the TT decomposition of a multidimensional identity matrix.
    all the TT ranks are 1.

    Args:
        shape (list[int]): the shape.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Returns:
        torchtt.TT: the one tensor.
    &#34;&#34;&#34;
    
    shape = list(shape)
    
    cores = [tn.unsqueeze(tn.unsqueeze(tn.eye(s, dtype=dtype, device = device),0),3) for s in shape]            
    
    return TT(cores)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.kron"><code class="name flex">
<span>def <span class="ident">kron</span></span>(<span>first, second)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the tensor Kronecker product.
If None is provided as input the reult is the other tensor.
If A is N_1 x &hellip; x N_d and B is M_1 x &hellip; x M_p, then kron(A,B) is N_1 x &hellip; x N_d x M_1 x &hellip; x M_p</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>first</code></strong> :&ensp;<code>torchtt.TT</code> or <code>None</code></dt>
<dd>first argument.</dd>
<dt><strong><code>second</code></strong> :&ensp;<code>torchtt.TT</code> or <code>none</code></dt>
<dd>second argument.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>IncompatibleTypes</code></dt>
<dd>Incompatible data types (make sure both are either TT-matrices or TT-tensors).</dd>
<dt><code>InvalidArguments</code></dt>
<dd>Invalid arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the result.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kron(first, second):
    &#34;&#34;&#34;
    Computes the tensor Kronecker product.
    If None is provided as input the reult is the other tensor.
    If A is N_1 x ... x N_d and B is M_1 x ... x M_p, then kron(A,B) is N_1 x ... x N_d x M_1 x ... x M_p


    Args:
        first (torchtt.TT or None): first argument.
        second (torchtt.TT or none): second argument.

    Raises:
        IncompatibleTypes: Incompatible data types (make sure both are either TT-matrices or TT-tensors).
        InvalidArguments: Invalid arguments.

    Returns:
        torchtt.TT: the result.
    &#34;&#34;&#34;
    if first == None and isinstance(second,TT):
        cores_new = [c.clone() for c in second.cores]
        result = TT(cores_new)
    elif second == None and isinstance(first,TT): 
        cores_new = [c.clone() for c in first.cores]
        result = TT(cores_new)
    elif isinstance(first,TT) and isinstance(second,TT):
        if first.is_ttm != second.is_ttm:
            raise IncompatibleTypes(&#39;Incompatible data types (make sure both are either TT-matrices or TT-tensors).&#39;)
    
        # concatenate the result
        cores_new = [c.clone() for c in first.cores] + [c.clone() for c in second.cores]
        result = TT(cores_new)
    else:
        raise InvalidArguments(&#39;Invalid arguments.&#39;)
    return result</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>vectors)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a meshgrid of torchtt.TT objects. Similar to numpy.meshgrid or torch.meshgrid.
The input is a list of d torch.tensor vectors of sizes N_1, &hellip; ,N_d
The result is a list of torchtt.TT instances of shapes N1 x &hellip; x Nd.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vectors</code></strong> :&ensp;<code>list[torch.tensor]</code></dt>
<dd>the vectors (1d tensors).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[torchtt.TT]</code></dt>
<dd>the resulting meshgrid.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(vectors):
    &#34;&#34;&#34;
    Creates a meshgrid of torchtt.TT objects. Similar to numpy.meshgrid or torch.meshgrid.
    The input is a list of d torch.tensor vectors of sizes N_1, ... ,N_d
    The result is a list of torchtt.TT instances of shapes N1 x ... x Nd.
    
    Args:
        vectors (list[torch.tensor]): the vectors (1d tensors).

    Returns:
        list[torchtt.TT]: the resulting meshgrid.
    &#34;&#34;&#34;
    
    Xs = []
    dtype = vectors[0].dtype
    for i in range(len(vectors)):
        lst = [tn.ones((1,v.shape[0],1),dtype=dtype) for v in vectors]
        lst[i] = tn.reshape(vectors[i],[1,-1,1])
        Xs.append(TT(lst))
    return Xs</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.numel"><code class="name flex">
<span>def <span class="ident">numel</span></span>(<span>tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the number of entries needed to store the TT cores for the given tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong> :&ensp;<code>torchtt.TT</code></dt>
<dd>the TT representation of the tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>number of floats stored for the TT decomposition.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numel(tensor):
    &#34;&#34;&#34;
    Return the number of entries needed to store the TT cores for the given tensor.

    Args:
        tensor (torchtt.TT): the TT representation of the tensor.

    Returns:
        int: number of floats stored for the TT decomposition.
    &#34;&#34;&#34;
    
    return sum([tn.numel(tensor.cores[i]) for i in range(len(tensor.N))])</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.ones"><code class="name flex">
<span>def <span class="ident">ones</span></span>(<span>shape, dtype=torch.float64, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Construct a tensor that contains only ones.
the shape can be a list of ints or a list of tuples of ints. The second case creates a TT matrix.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>list[int]</code> or <code>list[tuple[int]]</code></dt>
<dd>the shape.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>torch.dtype</code>, optional</dt>
<dd>the dtype of the returned tensor. Defaults to tn.float64.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code>, optional</dt>
<dd>the device where the TT cores are created (None means CPU). Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Shape must be a list.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the one tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones(shape, dtype=tn.float64, device = None):
    &#34;&#34;&#34;
    Construct a tensor that contains only ones.
    the shape can be a list of ints or a list of tuples of ints. The second case creates a TT matrix.

    Args:
        shape (list[int] or list[tuple[int]]): the shape.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Raises:
        InvalidArguments: Shape must be a list.

    Returns:
        torchtt.TT: the one tensor.
    &#34;&#34;&#34;
    if isinstance(shape,list):
        d = len(shape)
        if d==0:
            return TT(None)
        else:
            if isinstance(shape[0],tuple):
                # we create a TT-matrix
                cores = [tn.ones([1,shape[i][0],shape[i][1],1],dtype=dtype,device=device) for i in range(d)]            
                
            else:
                # we create a TT-tensor
                cores = [tn.ones([1,shape[i],1],dtype=dtype,device=device) for i in range(d)]
            
    else:
        raise InvalidArguments(&#39;Shape must be a list.&#39;)
    
    return TT(cores)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.randn"><code class="name flex">
<span>def <span class="ident">randn</span></span>(<span>N, R, var=1.0, dtype=torch.float64, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A torchtt.TT tensor of shape N = [N1 x &hellip; x Nd] and rank R is returned.
The entries of the fuill tensor are alomst normal distributed with the variance var.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>the shape.</dd>
<dt><strong><code>R</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>the rank.</dd>
<dt><strong><code>var</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>the variance. Defaults to 1.0.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>torch.dtype</code>, optional</dt>
<dd>the dtype of the returned tensor. Defaults to tn.float64.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code>, optional</dt>
<dd>the device where the TT cores are created (None means CPU). Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the result.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def randn(N, R, var = 1.0, dtype = tn.float64, device = None):
    &#34;&#34;&#34;
    A torchtt.TT tensor of shape N = [N1 x ... x Nd] and rank R is returned. 
    The entries of the fuill tensor are alomst normal distributed with the variance var.
    
    Args:
        N (list[int]): the shape.
        R (list[int]): the rank.
        var (float, optional): the variance. Defaults to 1.0.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Returns:
        torchtt.TT: the result.
    &#34;&#34;&#34;

    d = len(N)
    v1 = var / np.prod(R)
    v = v1**(1/d)
    cores = [None] * d
    for i in range(d):
        cores[i] = tn.randn([R[i],N[i][0],N[i][1],R[i+1]] if isinstance(N[i],tuple) else [R[i],N[i],R[i+1]], dtype = dtype, device = device)*np.sqrt(v)

    return TT(cores)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.random"><code class="name flex">
<span>def <span class="ident">random</span></span>(<span>N, R, dtype=torch.float64, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a tensor of shape N with random cores of rank R.
Each core is a normal distributed with mean 0 and variance 1.
Check also the method torchtt.randn()for better random tensors in the TT format.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>list[int]</code> or <code>list[tuple[int]]</code></dt>
<dd>the shape of the tensor. If the elements are tuples of integers, we deal with a TT-matrix.</dd>
<dt><strong><code>R</code></strong> :&ensp;<code>list[int]</code> or <code>int</code></dt>
<dd>can be a list if the exact rank is specified or an integer if the maximum rank is secified.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>torch.dtype</code>, optional</dt>
<dd>the dtype of the returned tensor. Defaults to tn.float64.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code>, optional</dt>
<dd>the device where the TT cores are created (None means CPU). Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Check if N and R are right.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the result.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random(N, R, dtype = tn.float64, device = None):
    &#34;&#34;&#34;
    Returns a tensor of shape N with random cores of rank R.
    Each core is a normal distributed with mean 0 and variance 1.
    Check also the method torchtt.randn()for better random tensors in the TT format.

    Args:
        N (list[int] or list[tuple[int]]): the shape of the tensor. If the elements are tuples of integers, we deal with a TT-matrix.
        R (list[int] or int): can be a list if the exact rank is specified or an integer if the maximum rank is secified.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Raises:
        InvalidArguments: Check if N and R are right.

    Returns:
        torchtt.TT: the result.
    &#34;&#34;&#34;
    
    if isinstance(R,int):
        R = [1]+[R]*(len(N)-1)+[1]
    elif len(N)+1 != len(R) or R[0] != 1 or R[-1] != 1 or len(N)==0:
        raise InvalidArguments(&#39;Check if N and R are right.&#39;)
        
    cores = []
    
    for i in range(len(N)):
        cores.append(tn.randn([R[i],N[i][0],N[i][1],R[i+1]] if isinstance(N[i],tuple) else [R[i],N[i],R[i+1]], dtype = dtype, device = device))
        
    T = TT(cores)
    
    return T</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.rank1TT"><code class="name flex">
<span>def <span class="ident">rank1TT</span></span>(<span>vectors)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the rank 1 TT from a list of vectors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vectors</code></strong> :&ensp;<code>list[torch.tensor]</code></dt>
<dd>the list of vectors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the resulting TT object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rank1TT(vectors):
    &#34;&#34;&#34;
    Compute the rank 1 TT from a list of vectors.

    Args:
        vectors (list[torch.tensor]): the list of vectors.

    Returns:
        torchtt.TT: the resulting TT object.
    &#34;&#34;&#34;
    
    return TT([tn.reshape(vectors[i],[1,-1,1]) for i in range(len(vectors))])</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.reshape"><code class="name flex">
<span>def <span class="ident">reshape</span></span>(<span>tens, shape, eps=1e-16, rmax=2048)</span>
</code></dt>
<dd>
<div class="desc"><p>Reshapes a torchtt.TT tensor in the TT format.
A rounding is also performed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tens</code></strong> :&ensp;<code>torchtt.TT</code></dt>
<dd>the input tensor.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>list[int]</code> or <code>list[tuple[int]]</code></dt>
<dd>the desired shape. In the case of a TT operator the shape has to be given as list of tuples of ints [(M1,N1),&hellip;,(Md,Nd)].</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>relative accuracy. Defaults to 1e-16.</dd>
<dt><strong><code>rmax</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>maximum rank. Defaults to 2048.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ShapeMismatch</code></dt>
<dd>Tthe product of modes should remain equal. Check the given shape.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the resulting tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reshape(tens, shape, eps = 1e-16, rmax = 2048):
    &#34;&#34;&#34;
    Reshapes a torchtt.TT tensor in the TT format.
    A rounding is also performed.
    
    Args:
        tens (torchtt.TT): the input tensor.
        shape (list[int] or list[tuple[int]]): the desired shape. In the case of a TT operator the shape has to be given as list of tuples of ints [(M1,N1),...,(Md,Nd)].
        eps (float, optional): relative accuracy. Defaults to 1e-16.
        rmax (int, optional): maximum rank. Defaults to 2048.

    Raises:
        ShapeMismatch: Tthe product of modes should remain equal. Check the given shape.

    Returns:
        torchtt.TT: the resulting tensor.
    &#34;&#34;&#34;
    
    
    if tens.is_ttm:
        M = []
        N = []
        for t in shape:
            M.append(t[0])
            N.append(t[1])
        if np.prod(tens.N)!=np.prod(N) or np.prod(tens.M)!=np.prod(M):
            raise ShapeMismatch(&#39;Tthe product of modes should remain equal. Check the given shape.&#39;)
        core = tens.cores[0]
        cores_new = []
        
        idx = 0
        idx_shape = 0
        
        while True:
            if core.shape[1] % M[idx_shape] == 0 and core.shape[2] % N[idx_shape] == 0:
                if core.shape[1] // M[idx_shape] &gt; 1 and core.shape[2] // N[idx_shape] &gt; 1:
                    m1 = M[idx_shape]
                    m2 = core.shape[1] // m1
                    n1 = N[idx_shape]
                    n2 = core.shape[2] // n1
                    r1 = core.shape[0]
                    r2 = core.shape[-1]
                    tmp = tn.reshape(core,[r1*m1,m2,n1,n2*r2])
                    
                    crz,_ = mat_to_tt(tmp, [r1*m1,m2], [n1,n2*r2], eps, rmax)
                    
                    cores_new.append(tn.reshape(crz[0],[r1,m1,n1,-1]))
                    
                    core = tn.reshape(crz[1],[-1,m2,n2,r2]) 
                else:
                    cores_new.append(core+0)
                    if idx == len(tens.cores)-1:
                        break
                    else:
                        idx+=1
                        core = tens.cores[idx]
                idx_shape += 1
                if idx_shape == len(shape):
                    break
            else: 
                idx += 1
                if idx&gt;=len(tens.cores):
                    break
                
                core = tn.einsum(&#39;ijkl,lmno-&gt;ijmkno&#39;,core,tens.cores[idx])
                core = tn.reshape(core,[core.shape[0],core.shape[1]*core.shape[2],-1,core.shape[-1]])
                
    else:
        if np.prod(tens.N)!=np.prod(shape):
            raise ShapeMismatch(&#39;Tthe product of modes should remain equal. Check the given shape.&#39;)
            
        core = tens.cores[0]
        cores_new = []
        
        idx = 0
        idx_shape = 0
        while True:
            if core.shape[1] % shape[idx_shape] == 0:
                if core.shape[1] // shape[idx_shape] &gt; 1:
                    s1 = shape[idx_shape]
                    s2 = core.shape[1] // s1
                    r1 = core.shape[0]
                    r2 = core.shape[2]
                    tmp = tn.reshape(core,[r1*s1,s2*r2])
                    
                    crz,_ = to_tt(tmp,tmp.shape,eps,rmax)
                    
                    cores_new.append(tn.reshape(crz[0],[r1,s1,-1]))
                    
                    core = tn.reshape(crz[1],[-1,s2,r2]) 
                else:
                    cores_new.append(core+0)
                    if idx == len(tens.cores)-1:
                        break
                    else:
                        idx+=1
                        core = tens.cores[idx]
                idx_shape += 1
                if idx_shape == len(shape):
                    break
            else: 
                idx += 1
                if idx&gt;=len(tens.cores):
                    break
                
                core = tn.einsum(&#39;ijk,klm-&gt;ijlm&#39;,core,tens.cores[idx])
                core = tn.reshape(core,[core.shape[0],-1,core.shape[-1]])
                
    return TT(cores_new).round(eps)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.zeros"><code class="name flex">
<span>def <span class="ident">zeros</span></span>(<span>shape, dtype=torch.float64, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Construct a tensor that contains only ones.
the shape can be a list of ints or a list of tuples of ints. The second case creates a TT matrix.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>list[int]</code> or <code>list[tuple[int]]</code></dt>
<dd>the shape.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>torch.dtype</code>, optional</dt>
<dd>the dtype of the returned tensor. Defaults to tn.float64.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code>, optional</dt>
<dd>the device where the TT cores are created (None means CPU). Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Shape must be a list.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the one tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros(shape, dtype=tn.float64, device = None):
    &#34;&#34;&#34;
    Construct a tensor that contains only ones.
    the shape can be a list of ints or a list of tuples of ints. The second case creates a TT matrix.

    Args:
        shape (list[int] or list[tuple[int]]): the shape.
        dtype (torch.dtype, optional): the dtype of the returned tensor. Defaults to tn.float64.
        device (torch.device, optional): the device where the TT cores are created (None means CPU). Defaults to None.

    Raises:
        InvalidArguments: Shape must be a list.

    Returns:
        torchtt.TT: the one tensor.
    &#34;&#34;&#34;
    if isinstance(shape,list):
        d = len(shape)
        if isinstance(shape[0],tuple):
            # we create a TT-matrix
            cores = [tn.zeros([1,shape[i][0],shape[i][1],1],dtype=dtype, device = device) for i in range(d)]            
            
        else:
            # we create a TT-tensor
            cores = [tn.zeros([1,shape[i],1],dtype=dtype, device = device) for i in range(d)]
            
    else:
        raise InvalidArguments(&#39;Shape must be a list.&#39;)
    
    return TT(cores)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchtt.torchtt.TT"><code class="flex name class">
<span>class <span class="ident">TT</span></span>
<span>(</span><span>source, shape=None, eps=1e-10, rmax=2048)</span>
</code></dt>
<dd>
<div class="desc"><p>Constructor of the TT class. Can convert full tensor in the TT-format (from <code>torch.tensor</code> or <code>numpy.array</code>).
In the case of tensor operators of full shape <code>M1 x &hellip; Md x N1 x &hellip; x Nd</code>, the shape must be specified as a list of tuples <code>[(M1,N1),&hellip;,(Md,Nd)]</code>.
A TT-object can also be computed from cores if the list of cores is passed as argument.
If None is provided, an empty tensor is created.</p>
<p><span><span class="MathJax_Preview">\mathsf{x}=\sum\limits_{r_1...r_{d-1}=1}^{R_1,...,R_{d-1}} \mathsf{x}^{(1)}_{1i_1r_1}\cdots\mathsf{x}^{(d)}_{r_{d-1}i_d1}</span><script type="math/tex">\mathsf{x}=\sum\limits_{r_1...r_{d-1}=1}^{R_1,...,R_{d-1}} \mathsf{x}^{(1)}_{1i_1r_1}\cdots\mathsf{x}^{(d)}_{r_{d-1}i_d1}</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source</code></strong> :&ensp;<code>torch.tensor ot list[torch.tensor]</code> or <code>numpy.array</code> or <code>None</code></dt>
<dd>the input tensor in full format or the cores. If a torch.tensor or numpy array is provided</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>list[int]</code> or <code>list[tuple[int]]</code>, optional</dt>
<dd>the shape (if it differs from the one provided). For the TT-matrix case is mandatory. Defaults to None.</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>tolerance of the TT approximation. Defaults to 1e-10.</dd>
<dt><strong><code>rmax</code></strong> :&ensp;<code>int</code> or <code>list[int]</code>, optional</dt>
<dd>maximum rank (either a list of integer or an integer). Defaults to 1000.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RankMismatch</code></dt>
<dd>Ranks of the given cores do not match (chace the spaes of the cores).</dd>
<dt><code>InvalidArguments</code></dt>
<dd>Invalid input: TT-cores have to be either 4d or 3d.</dd>
<dt><code>InvalidArguments</code></dt>
<dd>Check the ranks and the mode size.</dd>
<dt><code>NotImplementedError</code></dt>
<dd>Function only implemented for torch tensors, numpy arrays, list of cores as torch tensors and None</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>import torchtt
import torch
x = torch.reshape(torch.arange(0,128,dtype = torch.float64),[8,4,4])
xtt = torchtt.TT(x)
ytt = torchtt.TT(torch.squeeze(x),[8,4,4])
A = torch.reshape(torch.arange(0,20160,dtype = torch.float64),[3,5,7,4,6,8])
Att = torchtt.TT(A,[(3,4),(5,6),(7,8)])
print(Att)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TT():
    
    def __init__(self, source, shape=None, eps=1e-10, rmax=2048):
        &#34;&#34;&#34;
        Constructor of the TT class. Can convert full tensor in the TT-format (from `torch.tensor` or `numpy.array`).
        In the case of tensor operators of full shape `M1 x ... Md x N1 x ... x Nd`, the shape must be specified as a list of tuples `[(M1,N1),...,(Md,Nd)]`.
        A TT-object can also be computed from cores if the list of cores is passed as argument.
        If None is provided, an empty tensor is created.
        
        \(\\mathsf{x}=\\sum\\limits_{r_1...r_{d-1}=1}^{R_1,...,R_{d-1}} \\mathsf{x}^{(1)}_{1i_1r_1}\\cdots\\mathsf{x}^{(d)}_{r_{d-1}i_d1}\)
   
        Args:
            source (torch.tensor ot list[torch.tensor] or numpy.array or None): the input tensor in full format or the cores. If a torch.tensor or numpy array is provided
            shape (list[int] or list[tuple[int]], optional): the shape (if it differs from the one provided). For the TT-matrix case is mandatory. Defaults to None.
            eps (float, optional): tolerance of the TT approximation. Defaults to 1e-10.
            rmax (int or list[int], optional): maximum rank (either a list of integer or an integer). Defaults to 1000.

        Raises:
            RankMismatch: Ranks of the given cores do not match (chace the spaes of the cores).
            InvalidArguments: Invalid input: TT-cores have to be either 4d or 3d.
            InvalidArguments: Check the ranks and the mode size.
            NotImplementedError: Function only implemented for torch tensors, numpy arrays, list of cores as torch tensors and None
        
        Examples:
            import torchtt
            import torch
            x = torch.reshape(torch.arange(0,128,dtype = torch.float64),[8,4,4])
            xtt = torchtt.TT(x)
            ytt = torchtt.TT(torch.squeeze(x),[8,4,4])
            A = torch.reshape(torch.arange(0,20160,dtype = torch.float64),[3,5,7,4,6,8])
            Att = torchtt.TT(A,[(3,4),(5,6),(7,8)])
            print(Att)        
        
        &#34;&#34;&#34;
       
        
        if source is None:
            # empty TT
            self.cores = []
            self.M = []
            self.N = []
            self.R = [1,1]
            self.is_ttm = False
            
        elif isinstance(source, list):
            # tt cores were passed directly
            
            # check if sizes are consistent
            prev = 1
            N = []
            M = []
            R = [source[0].shape[0]]
            d = len(source)
            for i in range(len(source)):
                s = source[i].shape
                
                if s[0] != R[-1]:
                    raise RankMismatch(&#34;Ranks of the given cores do not match (chace the spaes of the cores).&#34;)
                if len(s) == 3:
                    R.append(s[2])
                    N.append(s[1])
                elif len(s)==4:
                    R.append(s[3])
                    M.append(s[1])
                    N.append(s[2])
                else:
                    raise InvalidArguments(&#34;Invalid input: TT-cores have to be either 4d or 3d.&#34;)
            
            if len(N) != d or len(R) != d+1 or R[0] != 1 or R[-1] != 1 or (len(M)!=0 and len(M)!=len(N)) :
                raise InvalidArguments(&#34;Check the ranks and the mode size.&#34;)
            
            self.cores = source
            self.R = R
            self.N = N
            if len(M) == len(N):
                self.M = M
                self.is_ttm = True
            else:
                self.is_ttm = False
            self.shape = [ (m,n) for m,n in zip(self.M,self.N) ] if self.is_ttm else [n for n in self.N]     

        elif tn.is_tensor(source):
            if shape == None:
                # no size is given. Deduce it from the tensor. No TT-matrix in this case.
                self.N = list(source.shape)
                if len(self.N)&gt;1:
                    self.cores, self.R = to_tt(source,self.N,eps,rmax,is_sparse=False)
                else:    
                    self.cores = [tn.reshape(source,[1,self.N[0],1])]
                    self.R = [1,1]
                self.is_ttm = False
            elif isinstance(shape,list) and isinstance(shape[0],tuple):
                # if the size contains tuples, we have a TT-matrix.
                if len(shape) &gt; 1:
                    self.M = [s[0] for s in shape]
                    self.N = [s[1] for s in shape]
                    self.cores, self.R = mat_to_tt(source, self.M, self.N, eps, rmax)
                    self.is_ttm = True
                else:
                    self.M = [shape[0][0]]
                    self.N = [shape[0][1]]
                    self.cores, self.R = [tn.reshape(source,[1,shape[0][0],shape[0][1],1])], [1,1]
                    self.is_ttm = True
            else:
                # TT-decomposition with prescribed size
                # perform reshape first
                self.N = shape
                self.cores, self.R = to_tt(tn.reshape(source,shape),self.N,eps,rmax,is_sparse=False)
                self.is_ttm = False
            self.shape = [ (m,n) for m,n in zip(self.M,self.N) ] if self.is_ttm else [n for n in self.N]     

        elif isinstance(source, np.ndarray):
            source = tn.tensor(source) 
                    
            if shape == None:
                # no size is given. Deduce it from the tensor. No TT-matrix in this case.
                self.N = list(source.shape)
                if len(self.N)&gt;1:
                    self.cores, self.R = to_tt(source,self.N,eps,rmax,is_sparse=False)
                else:    
                    self.cores = [tn.reshape(source,[1,self.N[0],1])]
                    self.R = [1,1]
                self.is_ttm = False
            elif isinstance(shape,list) and isinstance(shape[0],tuple):
                # if the size contains tuples, we have a TT-matrix.
                self.M = [s[0] for s in shape]
                self.N = [s[1] for s in shape]
                self.cores, self.R = mat_to_tt(source, self.M, self.N, eps, rmax)
                self.is_ttm = True
            else:
                # TT-decomposition with prescribed size
                # perform reshape first
                self.N = shape
                self.cores, self.R = to_tt(tn.reshape(source,shape),self.N,eps,rmax,is_sparse=False)
                self.is_ttm = False
            self.shape = [ (m,n) for m,n in zip(self.M,self.N) ] if self.is_ttm else [n for n in self.N]     
        else:
            raise NotImplementedError(&#34;Function only implemented for torch tensors, numpy arrays, list of cores as torch tensors and None.&#34;)

    def cuda(self, device = None):
        &#34;&#34;&#34;
        Return a torchtt.TT object on the CUDA device by cloning all the cores on the GPU.

        Args:
            device (torch.device, optional): The CUDA device (None for CPU). Defaults to None.

        Returns:
            TT-oject: The TT-object. The TT-cores are on CUDA.
        &#34;&#34;&#34;
         
        t = TT(None)
        t.N = self.N.copy()
        t.R = self.R.copy()
        t.is_ttm = self.is_ttm
        if self.is_ttm:
            t.M = self.M.copy()
        t.cores = [ c.cuda(device) for c in self.cores]

        return t

    def cpu(self):
        &#34;&#34;&#34;
        Retrive the cores from the GPU.

        Returns:
            TT-object: The TT-object on CPU.
        &#34;&#34;&#34;

        t = TT(None)
        t.N = self.N.copy()
        t.R = self.R.copy()
        t.is_ttm = self.is_ttm
        if self.is_ttm:
            t.M = self.M.copy()
        t.cores = [ c.cpu() for c in self.cores]

        return t

    def is_cuda(self):
        &#34;&#34;&#34;
        Return True if the tensor is on GPU.

        Returns:
            bool: Is the torchtt.TT on GPU or not.
        &#34;&#34;&#34;
        return all([c.is_cuda for c in self.core])

    
    def to(self, device = None, dtype = None):
        &#34;&#34;&#34;
        Moves the TT instance to the given device with the given dtype.

        Args:
            device (torch.device, optional): The desired device. If none is provided, the device is the CPU. Defaults to None.
            dtype (torch.dtype, optional): The desired dtype (torch.float64, torch.float32,...). If none is provided the dtype is not changed. Defaults to None.
        &#34;&#34;&#34;
        t = TT(None)
        t.N = self.N.copy()
        t.R = self.R.copy()
        t.is_ttm = self.is_ttm
        if self.is_ttm:
            t.M = self.M.copy()
        t.cores = [ c.to(device=device,dtype=dtype) for c in self.cores]

        return t

    def detach(self):
        &#34;&#34;&#34;
        Detaches the TT tensor. Similar to torch.tensor.detach().

        Returns:
            torchtt.TT: the detached tensor.
        &#34;&#34;&#34;
        return TT([c.detach() for c in self.cores])
        
    def clone(self):
        &#34;&#34;&#34;
        Clones the torchtt.TT instance. Similat to torch.tensor.clone().

        Returns:
            torchtt.TT: the cloned TT object.
        &#34;&#34;&#34;
        return TT([c.clone() for c in self.cores]) 

    def full(self):       
        &#34;&#34;&#34;
        Return the full tensor.
        In case of a TTM, the result has the shape M1 x M2 x ... x Md x N1 x N2 x ... x Nd.

        Returns:
            torch.tensor: the full tensor.
        &#34;&#34;&#34;
        if self.is_ttm:
            # the case of tt-matrix
            tfull = self.cores[0][0,:,:,:]
            for i in  range(1,len(self.cores)-1) :
                tfull = tn.einsum(&#39;...i,ijkl-&gt;...jkl&#39;,tfull,self.cores[i])
            if len(self.N) != 1:
                tfull = tn.einsum(&#39;...i,ijk-&gt;...jk&#39;,tfull,self.cores[-1][:,:,:,0])
                tfull = tn.permute(tfull,list(np.arange(len(self.N))*2)+list(np.arange(len(self.N))*2+1))
            else:
                tfull = tfull[:,:,0]
        else:
            # the case of a normal tt
            tfull = self.cores[0][0,:,:]
            for i in  range(1,len(self.cores)-1) :
                tfull = tn.einsum(&#39;...i,ijk-&gt;...jk&#39;,tfull,self.cores[i])
            if len(self.N) != 1:
                tfull = tn.einsum(&#39;...i,ij-&gt;...j&#39;,tfull,self.cores[-1][:,:,0])
            else:
                tfull = tn.squeeze(tfull)
        return tfull
    
    def numpy(self):
        &#34;&#34;&#34;
        Return the full tensor as a numpy.array.
        In case of a TTM, the result has the shape M1 x M2 x ... x Md x N1 x N2 x ... x Nd.
        If it is involved in an AD graph, an error will occur.
        
        Returns:
            numpy.array: the full tensor in numpy.
        &#34;&#34;&#34;
        return self.full().cpu().numpy()
    
    def __repr__(self):
        &#34;&#34;&#34;
        Show the information as a string

        Returns:
            string: the string representation of a torchtt.TT
        &#34;&#34;&#34;
        
        if self.is_ttm:
            output = &#39;TT-matrix&#39; 
            output += &#39; with sizes and ranks:\n&#39;
            output += &#39;M = &#39; + str(self.M) + &#39;\nN = &#39; + str(self.N) + &#39;\n&#39;
            output += &#39;R = &#39; + str(self.R) + &#39;\n&#39;
            output += &#39;Device: &#39;+str(self.cores[0].device)+&#39;, dtype: &#39;+str(self.cores[0].dtype)+&#39;\n&#39;
            entries = sum([tn.numel(c)  for c in self.cores])
            output += &#39;#entries &#39; + str(entries) +&#39; compression &#39; + str(entries/np.prod(np.array(self.N,dtype=np.float64)*np.array(self.M,dtype=np.float64))) +  &#39;\n&#39;
        else:
            output = &#39;TT&#39;
            output += &#39; with sizes and ranks:\n&#39;
            output += &#39;N = &#39; + str(self.N) + &#39;\n&#39;
            output += &#39;R = &#39; + str(self.R) + &#39;\n\n&#39;
            output += &#39;Device: &#39;+str(self.cores[0].device)+&#39;, dtype: &#39;+str(self.cores[0].dtype)+&#39;\n&#39;
            entries = sum([tn.numel(c) for c in self.cores])
            output += &#39;#entries &#39; + str(entries) +&#39; compression &#39;  + str(entries/np.prod(np.array(self.N,dtype=np.float64))) + &#39;\n&#39;
        
        return output
    
    def __radd__(self,other):
        &#34;&#34;&#34;
        Addition in the TT format. Implements the &#34;+&#34; operator. This function is called in the case a non-torchtt.TT object is added to the left.

        Args:
            other (int or float or torch.tensor scalar): the left operand.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        return self.__add__(other)

    def __add__(self,other):
        &#34;&#34;&#34;
        Addition in the TT format. Implements the &#34;+&#34; operator. The following type pairs are supported:
            - both operands are TT-tensors.
            - both operands are TT-matrices.
            - first operand is a TT-tensor or a TT-matrix and the second is a scalar (either torch.tensor scalar or int or float).

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): second operand.
            
        Raises:
            ShapeMismatch: Dimension mismatch.
            IncompatibleTypes: Addition between a tensor and a matrix is not defined.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;

        if np.isscalar(other) or ( tn.is_tensor(other) and tn.numel(other) == 1):
            # the second term is a scalar
            cores =  []
            
            for i in range(len(self.N)):
                if self.is_ttm:
                    pad1 = (0,0 if i == len(self.N)-1 else 1 , 0,0 , 0,0 , 0,0 if i==0 else 1)
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0,0 , 0 if i==0 else self.R[i],0)
                    othr = tn.ones([1,1,1,1],dtype=self.cores[i].dtype) * (other if i ==0 else 1)
                else:
                    pad1 = (0,0 if i == len(self.N)-1 else 1 , 0,0 , 0,0 if i==0 else 1)
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0 if i==0 else self.R[i],0)
                    othr = tn.ones([1,1,1],dtype=self.cores[i].dtype) * (other if i ==0 else 1)
                

                cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(othr,pad2))

                
            result = TT(cores)
        elif isinstance(other,TT):
        #second term is TT object 
            if self.is_ttm and other.is_ttm:
                # both are TT-matrices
                if self.M != self.M or self.N != self.N:
                    raise ShapeMismatch(&#39;Dimension mismatch.&#39;)
                    
                cores = []
                for i in range(len(self.N)):
                    pad1 = (0,0 if i == len(self.N)-1 else other.R[i+1], 0,0 , 0,0 , 0,0 if i==0 else other.R[i])
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0,0 , 0 if i==0 else self.R[i],0)
                    cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(other.cores[i],pad2))
                    
                result = TT(cores)
                
            elif self.is_ttm==False and other.is_ttm==False:
                # normal tensors in TT format.
                if self.N != self.N:
                    raise ShapeMismatch(&#39;Dimension mismatch.&#39;)
                    
                cores = []
                for i in range(len(self.N)):
                    pad1 = (0,0 if i == len(self.N)-1 else other.R[i+1] , 0,0 , 0,0 if i==0 else other.R[i])
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0 if i==0 else self.R[i],0)
                    cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(other.cores[i],pad2))
                    
                    
                result = TT(cores)
                
                
            else:
                # incompatible types 
                raise IncompatibleTypes(&#39;Addition between a tensor and a matrix is not defined.&#39;)
        else:
            InvalidArguments(&#39;Second term is incompatible.&#39;)
            
        return result
    
    def __rsub__(self,other):
        &#34;&#34;&#34;
        Subtract 2 tensors in the TT format. Implements the &#34;-&#34; operator.  

        Args:
            other (int or float or torch.tensor with 1 element): the first operand.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        T = self.__sub__(other)
        T.cores[0] = -T.cores[0]
        return T
    
    def __sub__(self,other):
        &#34;&#34;&#34;
        Subtract 2 tensors in the TT format. Implements the &#34;-&#34; operator.
        Possible second operands are: torchtt.TT, float, int, torch.tensor with 1 element.

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the second operand.

        Raises:
            ShapeMismatch: Both dimensions of the TT matrix should be equal.
            ShapeMismatch: Dimension mismatch.
            IncompatibleTypes: Addition between a tensor and a matrix is not defined.
            InvalidArguments: Second term is incompatible (must be either torchtt.TT or int or float or torch.tensor with 1 element).

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        if np.isscalar(other) or ( tn.is_tensor(other) and other.shape == []):
            # the second term is a scalar
            cores =  []
            
            for i in range(len(self.N)):
                if self.is_ttm:
                    pad1 = (0,0 if i == len(self.N)-1 else 1 , 0,0 , 0,0 , 0,0 if i==0 else 1)
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0,0 , 0 if i==0 else self.R[i],0)
                    othr = tn.ones([1,1,1,1],dtype=self.cores[i].dtype) * (-other if i ==0 else 1)
                else:
                    pad1 = (0,0 if i == len(self.N)-1 else 1 , 0,0 , 0,0 if i==0 else 1)
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0 if i==0 else self.R[i],0)
                    othr = tn.ones([1,1,1],dtype=self.cores[i].dtype) * (-other if i ==0 else 1)
                cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(othr,pad2))
            result = TT(cores)

        elif isinstance(other,TT):
        #second term is TT object 
            if self.is_ttm and other.is_ttm:
                # both are TT-matrices
                if self.M != self.M or self.N != self.N:
                    raise ShapeMismatch(&#39;Both dimensions of the TT matrix should be equal.&#39;)
                    
                cores = []
                for i in range(len(self.N)):
                    pad1 = (0,0 if i == len(self.N)-1 else other.R[i+1] , 0,0 , 0,0 , 0,0 if i==0 else other.R[i])
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0,0 , 0 if i==0 else self.R[i],0)
                    cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(-other.cores[i] if i==0 else other.cores[i],pad2))
                    
                result = TT(cores)
                
            elif self.is_ttm==False and other.is_ttm==False:
                # normal tensors in TT format.
                if self.N != self.N:
                    raise ShapeMismatch(&#39;Dimension mismatch.&#39;)
                    
                cores = []
                for i in range(len(self.N)):
                    pad1 = (0,0 if i == len(self.N)-1 else other.R[i+1] , 0,0 , 0,0 if i==0 else other.R[i])
                    pad2 = (0 if i == len(self.N)-1 else self.R[i+1],0 , 0,0 , 0 if i==0 else self.R[i],0)
                    cores.append(tnf.pad(self.cores[i],pad1)+tnf.pad(-other.cores[i] if i==0 else other.cores[i],pad2))
                    
                    
                result = TT(cores)
                
                
            else:
                # incompatible types 
                raise IncompatibleTypes(&#39;Addition between a tensor and a matrix is not defined.&#39;)
        else:
            InvalidArguments(&#39;Second term is incompatible (must be either torchtt.TT or int or float or torch.tensor with 1 element).&#39;)
            
        return result
    
    def __rmul__(self,other):
        &#34;&#34;&#34;
        Elementwise multiplication in the TT format.
        This implements the &#34;*&#34; operator when the left operand is not torchtt.TT.
        Following are supported:
         - TT tensor and TT tensor
         - TT matrix and TT matrix
         - TT tensor and scalar(int, float or torch.tensor scalar)

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the second operand.

        Raises:
            ShapeMismatch: Shapes must be equal.
            IncompatibleTypes: Second operand must be the same type as the fisrt (both should be either TT matrices or TT tensors).
            InvalidArguments: Second operand must be of type: torchtt.TT, float, int of torch.tensor.

        Returns:
            torchtt.TT: [description]
        &#34;&#34;&#34;
        
        return self.__mul__(other)
        
    def __mul__(self,other):
        &#34;&#34;&#34;
        Elementwise multiplication in the TT format.
        This implements the &#34;*&#34; operator.
        Following are supported:
         - TT tensor and TT tensor
         - TT matrix and TT matrix
         - TT tensor and scalar(int, float or torch.tensor scalar)

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the second operand.

        Raises:
            ShapeMismatch: Shapes must be equal.
            IncompatibleTypes: Second operand must be the same type as the fisrt (both should be either TT matrices or TT tensors).
            InvalidArguments: Second operand must be of type: torchtt.TT, float, int of torch.tensor.

        Returns:
            torchtt.TT: [description]
        &#34;&#34;&#34;
       
        # elementwise multiplication
        if isinstance(other, TT):
            if self.is_ttm and other.is_ttm:
                if self.N != other.N or self.M != other.M:
                    raise ShapeMismatch(&#39;Shapes must be equal.&#39;) 
                    
                cores_new = []
                
                for i in range(len(self.cores)):
                    core = tn.reshape(tn.einsum(&#39;aijb,mijn-&gt;amijbn&#39;,self.cores[i],other.cores[i]),[self.R[i]*other.R[i],self.M[i],self.N[i],self.R[i+1]*other.R[i+1]])
                    cores_new.append(core)
    
            elif self.is_ttm == False and other.is_ttm == False:
                if self.N != other.N:
                    raise ShapeMismatch(&#39;Shapes must be equal.&#39;) 
                    
                cores_new = []
                
                for i in range(len(self.cores)):
                    core = tn.reshape(tn.einsum(&#39;aib,min-&gt;amibn&#39;,self.cores[i],other.cores[i]),[self.R[i]*other.R[i],self.N[i],self.R[i+1]*other.R[i+1]])
                    cores_new.append(core)
            else:
                raise IncompatibleTypes(&#39;Second operand must be the same type as the fisrt (both should be either TT matrices or TT tensors).&#39;)
        elif isinstance(other,int) or isinstance(other,float) or isinstance(other,tn.tensor):
            cores_new = [c+0 for c in self.cores]
            cores_new[0] *= other
        else:
            raise InvalidArguments(&#39;Second operand must be of type: TT, float, int of tensorflow Tensor.&#39;)
        result = TT(cores_new)            
        return result
        
    def __matmul__(self,other):
        &#34;&#34;&#34;
        Matrix-vector multiplication in TT-format
        Suported operands:
            - TT-matrix @ TT-tensor -&gt; TT-tensor: y_i = A_ij * x_j
            - TT-tensor @ TT-matrix -&gt; TT-tensor: y_j = x_i * A_ij 
            - TT-matrix @ TT-matrix -&gt; TT-matrix: Y_ij = A_ik * B_kj
            - TT-matrix @ torch.tensor -&gt; torch.tensor: y_bi = A_ij * x_bj 
        In the last case, the multiplication is performed along the last modes and a full torch.tensor is returned.

        Args:
            other (torchtt.TT or torch.tensor): the second operand.

        Raises:
            ShapeMismatch: Shapes do not match.
            InvalidArguments: Wrong arguments.

        Returns:
            torchtt.TT or torch.tensor: the result. Can be full tensor if the second operand is full tensor.
        &#34;&#34;&#34;
     
        if self.is_ttm and tn.is_tensor(other):
            if self.N != list(other.shape)[-len(self.N):]:
                raise ShapeMismatch(&#34;Shapes do not match.&#34;)
            result = dense_matvec(self.cores,other) 
            return result

        elif self.is_ttm and other.is_ttm == False:
            # matrix-vector multiplication
            if self.N != other.N:
                raise ShapeMismatch(&#34;Shapes do not match.&#34;)
                
            cores_new = []
            
            for i in range(len(self.cores)):
                core = tn.reshape(tn.einsum(&#39;ijkl,mkp-&gt;imjlp&#39;,self.cores[i],other.cores[i]),[self.cores[i].shape[0]*other.cores[i].shape[0],self.cores[i].shape[1],self.cores[i].shape[3]*other.cores[i].shape[2]])
                cores_new.append(core)
            
            
        elif self.is_ttm and other.is_ttm:
            # multiplication between 2 TT-matrices
            if self.N != other.M:
                raise ShapeMismatch(&#34;Shapes do not match.&#34;)
                
            cores_new = []
            
            for i in range(len(self.cores)):
                core = tn.reshape(tn.einsum(&#39;ijkl,mknp-&gt;imjnlp&#39;,self.cores[i],other.cores[i]),[self.cores[i].shape[0]*other.cores[i].shape[0],self.cores[i].shape[1],other.cores[i].shape[2],self.cores[i].shape[3]*other.cores[i].shape[3]])
                cores_new.append(core)
        elif self.is_ttm == False and other.is_ttm:
            # vector-matrix multiplication
            if self.N != other.M:
                raise ShapeMismatch(&#34;Shapes do not match.&#34;)
                
            cores_new = []
            
            for i in range(len(self.cores)):
                core = tn.reshape(tn.einsum(&#39;mkp,ikjl-&gt;imjlp&#39;,self.cores[i],other.cores[i]),[self.cores[i].shape[0]*other.cores[i].shape[0],other.cores[i].shape[2],self.cores[i].shape[2]*other.cores[i].shape[3]])
                cores_new.append(core)
        else:
            raise InvalidArguments(&#34;Wrong arguments.&#34;)
            
        result = TT(cores_new)
        return result

    def fast_matvec(self,other, eps = 1e-12, nswp = 20, verb = False):
        &#34;&#34;&#34;
        Fast matrix vector multiplication A@x using DMRG iterations. Faster than traditional matvec + rounding.

        Args:
            other (torchtt.TT): the TT tensor.
            eps (float, optional): relative accuracy for DMRG. Defaults to 1e-12.
            nswp (int, optional): number of DMRG iterations. Defaults to 40.
            verb (bool, optional): show info for debug. Defaults to False.

        Raises:
            InvalidArguments: Second operand has to be TT object.
            IncompatibleTypes: First operand should be a TT matrix and second a TT vector.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        if not isinstance(other,TT):
            raise InvalidArguments(&#39;Second operand has to be TT object.&#39;)
        if not self.is_ttm or other.is_ttm:
            raise IncompatibleTypes(&#39;First operand should be a TT matrix and second a TT vector.&#39;)
            
        return dmrg_matvec(self, other, eps = eps, verb = verb, nswp = nswp)

    def apply_mask(self,indices):
        &#34;&#34;&#34;
        Evaluate the tensor on the given index list.

        Args:
            indices (list[list[int]]): the index list where the tensor should be evaluated. Length is M.

        Returns:
            torch.tensor: the values of the tensor

        Examples:
            x = torchtt.random([10,12,14],[1,4,5,1])
            indices = torch.tensor([[0,0,0],[1,2,3],[1,1,1]])
            val = x.apply_mask(indices)
        &#34;&#34;&#34;
        result = apply_mask(self.cores,self.R,indices)
        return result

    def __truediv__(self,other):
        &#34;&#34;&#34;
        This function implements the &#34;/&#34; operator.
        This operation is performed using the AMEN solver. The number of sweeps and rthe relative accuracy are fixed.
        For most cases it is sufficient but sometimes it can fail.
        Check the function torchtt.elementwise_divide() if you want to change the arguments of the AMEN solver.
        Example: z = 1.0/x # x is TT instance

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the first operand.

        Raises:
            IncompatibleTypes: Operands should be either TT or TTM.
            ShapeMismatch: Both operands should have the same shape.
            InvalidArguments: Operand not permitted. A TT-object can be divided only with scalars.
            
        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        if isinstance(other,int) or isinstance(other,float) or tn.is_tensor(other):
            # divide by a scalar
            cores_new = self.cores.copy()
            cores_new[0] /= other
            result = TT(cores_new)
        elif isinstance(other,TT):
            if self.is_ttm != other.is_ttm:
                raise IncompatibleTypes(&#39;Operands should be either TT or TTM.&#39;)
            if self.N != other.N or (self.is_ttm and self.M != other.M):
                raise ShapeMismatch(&#34;Both operands should have the same shape.&#34;)
            result = TT(amen_divide(other,self,50,None,1e-12,500,verbose=False))       
        else:
            raise InvalidArguments(&#39;Operand not permitted. A TT-object can be divided only with scalars.&#39;)
            
       
        return result
    
    def __rtruediv__(self,other):
        &#34;&#34;&#34;
        Right true division. this function is called when a non TT object is divided by a TT object.
        This operation is performed using the AMEN solver. The number of sweeps and rthe relative accuracy are fixed.
        For most cases it is sufficient but sometimes it can fail.
        Check the function torchtt.elementwise_divide() if you want to change the arguments of the AMEN solver.
        Example: z = 1.0/x # x is TT instance

        Args:
            other (torchtt.TT or float or int or torch.tensor with 1 element): the first operand.

        Raises:
            InvalidArguments: The first operand must be int, float or 1d torch.tensor.
            
        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        if isinstance(other,int) or isinstance(other,float) or ( tn.is_tensor(other) and other.numel()==1):
            o = ones(self.N,dtype=self.cores[0].dtype,device = self.cores[0].device)
            o.cores[0] *= other
            cores_new = amen_divide(self,o,50,None,1e-12,500,verbose=False)
        else:
            raise InvalidArguments(&#34;The first operand must be int, float or 1d torch.tensor.&#34;)   
         
        return TT(cores_new)

    
    
    def t(self):
        &#34;&#34;&#34;
        Returns the transpoise of a given TT matrix.
        
        Raises:
            InvalidArguments: Has to TT matrix.
            
        Returns: 
            torchtt.TT: the transpose. 
        &#34;&#34;&#34; 
        if not self.is_ttm:
            raise InvalidArguments(&#39;Has to TT matrix.&#39;)
            
        cores_new = [tn.permute(c,[0,2,1,3]) for c in self.cores]
        
        return TT(cores_new)
        
    
    def norm(self,squared=False):
        &#34;&#34;&#34;
        Computes the frobenius norm of a TT object.

        Args:
            squared (bool, optional): returns the square of the norm if True. Defaults to False.

        Returns:
            torch.tensor: the norm.
        &#34;&#34;&#34;
        
        if any([c.requires_grad or c.grad_fn != None for c in self.cores]):
            norm = tn.tensor([[1.0]],dtype = self.cores[0].dtype, device=self.cores[0].device)
            
            if self.is_ttm:
                for i in range(len(self.N)):
                    norm = tn.einsum(&#39;ab,aijm,bijn-&gt;mn&#39;,norm, self.cores[i], self.cores[i])
                norm = tn.squeeze(norm)
            else:
                           
                for i in range(len(self.N)):
                    norm = tn.einsum(&#39;ab,aim,bin-&gt;mn&#39;,norm, self.cores[i], self.cores[i])
                norm = tn.squeeze(norm)
            if squared:
                return norm
            else:
                return tn.sqrt(tn.abs(norm))
 
        else:        
            d = len(self.cores)

            core_now = self.cores[0]
            for i in range(d-1):
                if self.is_ttm:
                    mode_shape = [core_now.shape[1],core_now.shape[2]]
                    core_now = tn.reshape(core_now,[core_now.shape[0]*core_now.shape[1]*core_now.shape[2],-1])
                else:
                    mode_shape = [core_now.shape[1]]
                    core_now = tn.reshape(core_now,[core_now.shape[0]*core_now.shape[1],-1])
                    
                # perform QR
                Qmat, Rmat = QR(core_now)
                     
                # take next core
                core_next = self.cores[i+1]
                shape_next = list(core_next.shape[1:])
                core_next = tn.reshape(core_next,[core_next.shape[0],-1])
                core_next = Rmat @ core_next
                core_next = tn.reshape(core_next,[Qmat.shape[1]]+shape_next)
                
                # update the cores
                
                core_now = core_next
            if squared:
                return tn.linalg.norm(core_next)**2
            else:
                return tn.linalg.norm(core_next)

    def sum(self,index = None):
        &#34;&#34;&#34;
        Contracts a tensor in the TT format along the given indices and retuyrns the resulting tensor in the TT format.
        If no index list is given, the sum over all indices is performed.

        Args:
            index (int or list[int] or None, optional): the indices along which the summation is performed. None selects all of them. Defaults to None.

        Raises:
            InvalidArguments: Invalid index.

        Returns:
            torchtt.TT or torch.tensor: the result.
        &#34;&#34;&#34;
        
        if index != None and isinstance(index,int):
            index = [index]
        if not isinstance(index,list) and index != None:
            raise InvalidArguments(&#39;Invalid index.&#39;)
             
        if index == None: 
            # the case we need to sum over all modes
            if self.is_ttm:
                C = tn.reduce_sum(self.cores[0],[0,1,2])
                for i in range(1,len(self.N)):
                    C = tn.sum(tn.einsum(&#39;i,ijkl-&gt;jkl&#39;,C,self.cores[i]),[0,1])
                S = tn.sum(C)
            else:
                C = tn.sum(self.cores[0],[0,1])
                for i in range(1,len(self.N)):
                    C = tn.sum(tn.einsum(&#39;i,ijk-&gt;jk&#39;,C,self.cores[i]),0)
                S = tn.sum(C)
        else:
            # we return the TT-tensor with summed indices
            cores = []
            
            if self.is_ttm:
                tmp = [1,2]
            else:
                tmp = [1]
                
            for i in range(len(self.N)):
                if i in index:
                    C = tn.sum(self.cores[i], tmp, keepdim = True)
                    cores.append(C)
                else:
                    cores.append(self.cores[i])
                        
            S = TT(cores)
            S.reduce_dims()
            
        return S

    def to_ttm(self):
        &#34;&#34;&#34;
        Converts a TT-tensor to the TT-matrix format. In the tensor has the shape N1 x ... x Nd, the result has the shape 
        N1 x ... x Nd x 1 x ... x 1.
    
        Returns:
            torch.TT: the result
        &#34;&#34;&#34;

        cores_new = [tn.reshape(c,(c.shape[0],c.shape[1],1,c.shape[2])) for c in self.cores]
        return TT(cores_new)

    def reduce_dims(self):
        &#34;&#34;&#34;
        Reduces the size 1 modes of the TT-object.
        At least one mode should be larger than 1.

        Returns:
            None.
        &#34;&#34;&#34;
        
        # TODO: implement a version that reduces the rank also. by spliting the cores with modes 1 into 2 using the SVD.
        
        if self.is_ttm:
            cores_new = []
            
            for i in range(len(self.N)):
                
                if self.cores[i].shape[1] == 1 and self.cores[i].shape[2] == 1:
                    if self.cores[i].shape[0] &gt; self.cores[i].shape[3] or i == len(self.N)-1:
                        # multiply to the left
                        if len(cores_new) &gt; 0:
                            cores_new[-1] = tn.einsum(&#39;ijok,kl-&gt;ijol&#39;,cores_new[-1], self.cores[i][:,0,0,:])
                        else: 
                            # there is no core to the left. Multiply right.
                            if i != len(self.N)-1:
                                self.cores[i+1] = tn.einsum(&#39;ij,jkml-&gt;ikml&#39;, self.cores[i][:,0,0,:],self.cores[i+1])
                            else:
                                cores_new.append(self.cores[i])
                            
                    else:
                        # multiply to the right. Set the carry 
                        self.cores[i+1] = tn.einsum(&#39;ij,jkml-&gt;ikml&#39;,self.cores[i][:,0,0,:],self.cores[i+1])
                        
                else:
                    cores_new.append(self.cores[i])
                    
            # update the cores and ranks and shape
            self.N = []
            self.M = []
            self.R = [1]
            for i in range(len(cores_new)):
                self.N.append(cores_new[i].shape[2])
                self.M.append(cores_new[i].shape[1])
                self.R.append(cores_new[i].shape[3])
            self.cores = cores_new
        else:
            cores_new = []
            
            for i in range(len(self.N)):
                
                if self.cores[i].shape[1] == 1:
                    if self.cores[i].shape[0] &gt; self.cores[i].shape[2] or i == len(self.N)-1:
                        # multiply to the left
                        if len(cores_new) &gt; 0:
                            cores_new[-1] = tn.einsum(&#39;ijk,kl-&gt;ijl&#39;,cores_new[-1], self.cores[i][:,0,:])
                        else: 
                            # there is no core to the left. Multiply right.
                            if i != len(self.N)-1:
                                self.cores[i+1] = tn.einsum(&#39;ij,jkl-&gt;ikl&#39;, self.cores[i][:,0,:],self.cores[i+1])
                            else:
                                cores_new.append(self.cores[i])
                                
                            
                    else:
                        # multiply to the right. Set the carry 
                        self.cores[i+1] = tn.einsum(&#39;ij,jkl-&gt;ikl&#39;,self.cores[i][:,0,:],self.cores[i+1])
                        
                else:
                    cores_new.append(self.cores[i])
            
            
            # update the cores and ranks and shape
            self.N = []
            self.R = [1]
            for i in range(len(cores_new)):
                self.N.append(cores_new[i].shape[1])
                self.R.append(cores_new[i].shape[2])
            self.cores = cores_new
                    
                    
        self.shape = [ (m,n) for m,n in zip(self.M,self.N) ] if self.is_ttm else [n for n in self.N] 
        
    def __getitem__(self,index):
        &#34;&#34;&#34;
        Performs slicinf of a TT object.
        Both TT matrix and TT tensor are supported.
        Similar to pytorch or numpy slicing.

        Args:
            index (tuple[slice] or tuple[int] or int or Ellipsis or slice): the slicing.

        Raises:
            NotImplementedError: Ellipsis are not supported.
            InvalidArguments: Slice size is invalid.
            InvalidArguments: Invalid slice. Tensor is not 1d.


        Returns:
            torchtt.TT or torch.tensor: the result. If all the indices are fixed, a scalar torch.tensor is returned otherwise a torchtt.TT.
        &#34;&#34;&#34;
        
        
        # slicing function
        
        ##### TODO: include Ellipsis support.
        
        # if a slice containg integers is passed, an element is returned
        # if ranged slices are used, a TT-object has to be returned.
        
        if isinstance(index,tuple):
            # check if more than one Ellipsis are to be found.
            if index.count(Ellipsis) &gt; 0:
                raise NotImplementedError(&#39;Ellipsis are not supported.&#39;)
            if self.is_ttm:
                if len(index) != len(self.N)*2:
                    raise InvalidArguments(&#39;Slice size is invalid.&#39;)
                    
                cores_new = []
                for i in range(len(self.cores)):
                    # cores_new.append(self.cores[i][:,index[i],index[i+len(self.N)],:])
                    if isinstance(index[i],slice):
                        cores_new.append(self.cores[i][:,index[i],index[i+len(self.N)],:])
                    else:
                        cores_new.append(tn.reshape(self.cores[i][:,index[i],index[i+len(self.N)],:],[self.R[i],1,1,self.R[i+1]]))
               
                
            else:
                if len(index) != len(self.N):
                    raise InvalidArguments(&#39;Slice size is invalid.&#39;)
                    
                cores_new = []
                for i in range(len(self.cores)):
                    if isinstance(index[i],slice):
                        cores_new.append(self.cores[i][:,index[i],:])
                    else:
                        cores_new.append(tn.reshape(self.cores[i][:,index[i],:],[self.R[i],-1,self.R[i+1]]))
            
            sliced = TT(cores_new)
            sliced.reduce_dims()
            if (sliced.is_ttm == False and sliced.N == [1]) or (sliced.is_ttm and sliced.N == [1] and sliced.M == [1]):
                sliced = tn.squeeze(sliced.cores[0])
                
                
            # cores = None
            
            
        elif isinstance(index,int):
            # tensor is 1d and one element is retrived
            if len(self.N) == 1:
                sliced = self.cores[0][0,index,0]
            else:
                raise InvalidArguments(&#39;Invalid slice. Tensor is not 1d.&#39;)
                
            ## TODO
        elif isinstance(index,Ellipsis):
            # return a copy of the tensor
            sliced = TT([c.clone() for c in self.cores])
            
        elif isinstance(index,slice):
            # tensor is 1d and one slice is extracted
            if len(self.N) == 1:
                sliced = TT(self.cores[0][:,index,:])
            else:
                raise InvalidArguments(&#39;Invalid slice. Tensor is not 1d.&#39;)
            ## TODO
        else:
            raise InvalidArguments(&#39;Invalid slice.&#39;)
            
        
        return sliced
    
    def __pow__(self,other):
        &#34;&#34;&#34;
        Computes the tensor Kronecker product.
        This implements the &#34;**&#34; operator.
        If None is provided as input the reult is the other tensor.
        If A is N_1 x ... x N_d and B is M_1 x ... x M_p, then kron(A,B) is N_1 x ... x N_d x M_1 x ... x M_p


        Args:
            first (torchtt.TT or None): first argument.
            second (torchtt.TT or none): second argument.

        Raises:
            IncompatibleTypes: Incompatible data types (make sure both are either TT-matrices or TT-tensors).
            InvalidArguments: Invalid arguments.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        result = kron(self,other)
        
        return result
    
    def __rpow__(self,other):
        &#34;&#34;&#34;
        Computes the tensor Kronecker product.
        This implements the &#34;**&#34; operator.
        If None is provided as input the reult is the other tensor.
        If A is N_1 x ... x N_d and B is M_1 x ... x M_p, then kron(A,B) is N_1 x ... x N_d x M_1 x ... x M_p


        Args:
            first (torchtt.TT or None): first argument.
            second (torchtt.TT or none): second argument.

        Raises:
            IncompatibleTypes: Incompatible data types (make sure both are either TT-matrices or TT-tensors).
            InvalidArguments: Invalid arguments.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        result = kron(self,other)
        
        return result
    
    def __neg__(self):
        &#34;&#34;&#34;
        Returns the negative of a given TT tensor.
        This implements the unery operator &#34;-&#34;

        Returns:
            torchtt.TT: the negated tensor.
        &#34;&#34;&#34;
    
        cores_new = [c.clone() for c in self.cores]
        cores_new[0] = -cores_new[0]
        return TT(cores_new)
    
    def __pos__(self):
        &#34;&#34;&#34;
        Implements the unary &#34;+&#34; operator returning a copy o the tensor.

        Returns:
            torchtt.TT: the tensor clone.
        &#34;&#34;&#34;
        
        cores_new = [c.clone() for c in self.cores]

        return TT(cores_new)
    
    def round(self, eps=1e-12, rmax = 2048): 
        &#34;&#34;&#34;
        Implements the rounding operations within a given tolerance epsilon.
        The maximum rank is also provided.

        Args:
            eps (float, optional): the relative accuracy. Defaults to 1e-12.
            rmax (int, optional): the maximum rank. Defaults to 2048.

        Returns:
            torchtt.TT: the result.
        &#34;&#34;&#34;
        
        # rmax is not list
        if not isinstance(rmax,list):
            rmax = [1] + len(self.N)*[rmax] + [1]
            
        # call the round function
        tt_cores, R = round_tt(self.cores, self.R.copy(), eps, rmax,self.is_ttm)
        # creates a new TT and return it
        T = TT(tt_cores)
               
        return T
    
    def to_qtt(self, eps = 1e-12, mode_size = 2, rmax = 2048):
        &#34;&#34;&#34;
        Converts a tensor to the QTT format: N1 x N2 x ... x Nd -&gt; mode_size x mode_size x ... x mode_size.
        The product of the mode sizes should be a power of mode_size.
        The tensor in QTT can be converted back using the qtt_to_tens() method.

        Args:
            eps (flaot,optional): the accuracy. Defaults to 1e-12.
            mode_size (int, optional): the size of the modes. Defaults to 2.
            rmax (int): the maximum rank. Defaults to 2048.
            

        Raises:
            ShapeMismatch: Only quadratic TTM can be tranformed to QTT.
            ShapeMismatch: Reshaping error: check if the dimensions are powers of the desired mode size.

        Returns:
            torchtt.TT: the resulting reshaped tensor.
            
        Examples:
            import torchtt
            x = torchtt.random([16,8,64,128],[1,2,10,12,1])
            x_qtt = x.to_qtt()
            print(x_qtt)
            xf = x_qtt.qtt_to_tens(x.N) # a TT-rounding is recommended.
            
        &#34;&#34;&#34;
       
        cores_new = []
        if self.is_ttm:
            shape_new = []
            for i in range(len(self.N)):
                if self.N[i]!=self.M[i]:
                    raise ShapeMismatch(&#39;Only quadratic TTM can be tranformed to QTT.&#39;)
                if self.N[i]==mode_size**int(math.log(self.N[i],mode_size)):
                    shape_new += [(mode_size,mode_size)]*int(math.log(self.N[i],mode_size))
                else:
                    raise ShapeMismatch(&#39;Reshaping error: check if the dimensions are powers of the desired mode size:\r\ncore size &#39;+str(list(self.cores[i].shape))+&#39; cannot be reshaped.&#39;)
                
            result = reshape(self, shape_new, eps, rmax)
        else:
            for core in self.cores:
                if int(math.log(core.shape[1],mode_size))&gt;2:
                    Nnew = [core.shape[0]*mode_size]+[mode_size]*(int(math.log(core.shape[1],mode_size))-2)+[core.shape[2]*mode_size]
                    try:
                        core = tn.reshape(core,Nnew)
                    except:
                        raise ShapeMismatch(&#39;Reshaping error: check if the dimensions care powers of the desired mode size:\r\ncore size &#39;+str(list(core.shape))+&#39; cannot be reshaped to &#39;+str(Nnew))
                    cores,_ = to_tt(core,Nnew,eps,rmax,is_sparse=False)
                    cores_new.append(tn.reshape(cores[0],[-1,mode_size,cores[0].shape[-1]]))
                    cores_new += cores[1:-1]
                    cores_new.append(tn.reshape(cores[-1],[cores[-1].shape[0],mode_size,-1]))
                else: 
                    cores_new.append(core)
            result = TT(cores_new)
            
        return result
               
    def qtt_to_tens(self, original_shape):
        &#34;&#34;&#34;
        Transform a tensor back from QTT.

        Args:
            original_shape (list): the original shape.

        Raises:
            InvalidArguments: Original shape must be a list.
            ShapeMismatch: Mode sizes do not match.

        Returns:
            torchtt.TT: the folded tensor.
        &#34;&#34;&#34;
        
        if not isinstance(original_shape,list):
            raise InvalidArguments(&#34;Original shape must be a list.&#34;)

        core = None
        cores_new = []
        
        if self.is_ttm:
            pass
        else:
            k = 0
            for c in self.cores:
                if core==None:
                    core = c
                    so_far = core.shape[1]
                else:
                    core = tn.einsum(&#39;...i,ijk-&gt;...jk&#39;,core,c)
                    so_far *= c.shape[1]
                if so_far==original_shape[k]:
                    core = tn.reshape(core,[core.shape[0],-1,core.shape[-1]])
                    cores_new.append(core)
                    core = None
                    k += 1
            if k!= len(original_shape):
                raise ShapeMismatch(&#39;Mode sizes do not match.&#39;)
        return TT(cores_new)
    
    def mprod(self, factor_matrices, mode):
        &#34;&#34;&#34;
        n-mode product.

        Args:
            factor_matrices (torch.tensor or list[torch.tensor]): either a single matrix is directly provided or a list of matrices for product along multiple modes.
            mode (int or list[int]): the mode for the product. If factor_matrices is a torch.tensor then mode is an integer and the multiplication will be performed along a single mode.
                                     If factor_matrices is a list, the mode has to be list[int] of equal size.

        Raises:
            InvalidArguments: Invalid arguments.

        Returns:
            torchtt.TT: the result
        &#34;&#34;&#34;
    
        if isinstance(factor_matrices,list) and isinstance(mode, list):
            cores_new = [c.clone() for c in self.cores]
            for i in range(len(factor_matrices)):
                cores_new[mode[i]] =  tn.einsum(&#39;imjk,lj-&gt;imlk&#39;,cores_new[mode[i]],factor_matrices[i]) if self.is_ttm else tn.einsum(&#39;ijk,lj-&gt;ilk&#39;,cores_new[mode[i]],factor_matrices[i]) 
        elif isinstance(factor_matrices, tn.tensor) and isinstance(mode, int):
            cores_new = [c.clone() for c in self.cores]
            cores_new[mode] =  tn.einsum(&#39;imjk,lj-&gt;imlk&#39;,cores_new[mode],factor_matrices) if self.is_ttm else tn.einsum(&#39;ijk,lj-&gt;ilk&#39;,cores_new[mode],factor_matrices) 
        else:
            raise InvalidArguments(&#39;Invalid arguments.&#39;)
        
        return TT(cores_new)        </code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="torchtt.torchtt.TT.apply_mask"><code class="name flex">
<span>def <span class="ident">apply_mask</span></span>(<span>self, indices)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the tensor on the given index list.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>list[list[int]]</code></dt>
<dd>the index list where the tensor should be evaluated. Length is M.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>the values of the tensor</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>x = torchtt.random([10,12,14],[1,4,5,1])
indices = torch.tensor([[0,0,0],[1,2,3],[1,1,1]])
val = x.apply_mask(indices)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_mask(self,indices):
    &#34;&#34;&#34;
    Evaluate the tensor on the given index list.

    Args:
        indices (list[list[int]]): the index list where the tensor should be evaluated. Length is M.

    Returns:
        torch.tensor: the values of the tensor

    Examples:
        x = torchtt.random([10,12,14],[1,4,5,1])
        indices = torch.tensor([[0,0,0],[1,2,3],[1,1,1]])
        val = x.apply_mask(indices)
    &#34;&#34;&#34;
    result = apply_mask(self.cores,self.R,indices)
    return result</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.clone"><code class="name flex">
<span>def <span class="ident">clone</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Clones the torchtt.TT instance. Similat to torch.tensor.clone().</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the cloned TT object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clone(self):
    &#34;&#34;&#34;
    Clones the torchtt.TT instance. Similat to torch.tensor.clone().

    Returns:
        torchtt.TT: the cloned TT object.
    &#34;&#34;&#34;
    return TT([c.clone() for c in self.cores]) </code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.cpu"><code class="name flex">
<span>def <span class="ident">cpu</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrive the cores from the GPU.</p>
<h2 id="returns">Returns</h2>
<p>TT-object: The TT-object on CPU.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cpu(self):
    &#34;&#34;&#34;
    Retrive the cores from the GPU.

    Returns:
        TT-object: The TT-object on CPU.
    &#34;&#34;&#34;

    t = TT(None)
    t.N = self.N.copy()
    t.R = self.R.copy()
    t.is_ttm = self.is_ttm
    if self.is_ttm:
        t.M = self.M.copy()
    t.cores = [ c.cpu() for c in self.cores]

    return t</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.cuda"><code class="name flex">
<span>def <span class="ident">cuda</span></span>(<span>self, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a torchtt.TT object on the CUDA device by cloning all the cores on the GPU.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code>, optional</dt>
<dd>The CUDA device (None for CPU). Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>TT-oject: The TT-object. The TT-cores are on CUDA.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cuda(self, device = None):
    &#34;&#34;&#34;
    Return a torchtt.TT object on the CUDA device by cloning all the cores on the GPU.

    Args:
        device (torch.device, optional): The CUDA device (None for CPU). Defaults to None.

    Returns:
        TT-oject: The TT-object. The TT-cores are on CUDA.
    &#34;&#34;&#34;
     
    t = TT(None)
    t.N = self.N.copy()
    t.R = self.R.copy()
    t.is_ttm = self.is_ttm
    if self.is_ttm:
        t.M = self.M.copy()
    t.cores = [ c.cuda(device) for c in self.cores]

    return t</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.detach"><code class="name flex">
<span>def <span class="ident">detach</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Detaches the TT tensor. Similar to torch.tensor.detach().</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the detached tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detach(self):
    &#34;&#34;&#34;
    Detaches the TT tensor. Similar to torch.tensor.detach().

    Returns:
        torchtt.TT: the detached tensor.
    &#34;&#34;&#34;
    return TT([c.detach() for c in self.cores])</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.fast_matvec"><code class="name flex">
<span>def <span class="ident">fast_matvec</span></span>(<span>self, other, eps=1e-12, nswp=20, verb=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Fast matrix vector multiplication A@x using DMRG iterations. Faster than traditional matvec + rounding.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>other</code></strong> :&ensp;<code>torchtt.TT</code></dt>
<dd>the TT tensor.</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>relative accuracy for DMRG. Defaults to 1e-12.</dd>
<dt><strong><code>nswp</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of DMRG iterations. Defaults to 40.</dd>
<dt><strong><code>verb</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>show info for debug. Defaults to False.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Second operand has to be TT object.</dd>
<dt><code>IncompatibleTypes</code></dt>
<dd>First operand should be a TT matrix and second a TT vector.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the result.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fast_matvec(self,other, eps = 1e-12, nswp = 20, verb = False):
    &#34;&#34;&#34;
    Fast matrix vector multiplication A@x using DMRG iterations. Faster than traditional matvec + rounding.

    Args:
        other (torchtt.TT): the TT tensor.
        eps (float, optional): relative accuracy for DMRG. Defaults to 1e-12.
        nswp (int, optional): number of DMRG iterations. Defaults to 40.
        verb (bool, optional): show info for debug. Defaults to False.

    Raises:
        InvalidArguments: Second operand has to be TT object.
        IncompatibleTypes: First operand should be a TT matrix and second a TT vector.

    Returns:
        torchtt.TT: the result.
    &#34;&#34;&#34;
    
    if not isinstance(other,TT):
        raise InvalidArguments(&#39;Second operand has to be TT object.&#39;)
    if not self.is_ttm or other.is_ttm:
        raise IncompatibleTypes(&#39;First operand should be a TT matrix and second a TT vector.&#39;)
        
    return dmrg_matvec(self, other, eps = eps, verb = verb, nswp = nswp)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.full"><code class="name flex">
<span>def <span class="ident">full</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the full tensor.
In case of a TTM, the result has the shape M1 x M2 x &hellip; x Md x N1 x N2 x &hellip; x Nd.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>the full tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def full(self):       
    &#34;&#34;&#34;
    Return the full tensor.
    In case of a TTM, the result has the shape M1 x M2 x ... x Md x N1 x N2 x ... x Nd.

    Returns:
        torch.tensor: the full tensor.
    &#34;&#34;&#34;
    if self.is_ttm:
        # the case of tt-matrix
        tfull = self.cores[0][0,:,:,:]
        for i in  range(1,len(self.cores)-1) :
            tfull = tn.einsum(&#39;...i,ijkl-&gt;...jkl&#39;,tfull,self.cores[i])
        if len(self.N) != 1:
            tfull = tn.einsum(&#39;...i,ijk-&gt;...jk&#39;,tfull,self.cores[-1][:,:,:,0])
            tfull = tn.permute(tfull,list(np.arange(len(self.N))*2)+list(np.arange(len(self.N))*2+1))
        else:
            tfull = tfull[:,:,0]
    else:
        # the case of a normal tt
        tfull = self.cores[0][0,:,:]
        for i in  range(1,len(self.cores)-1) :
            tfull = tn.einsum(&#39;...i,ijk-&gt;...jk&#39;,tfull,self.cores[i])
        if len(self.N) != 1:
            tfull = tn.einsum(&#39;...i,ij-&gt;...j&#39;,tfull,self.cores[-1][:,:,0])
        else:
            tfull = tn.squeeze(tfull)
    return tfull</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.is_cuda"><code class="name flex">
<span>def <span class="ident">is_cuda</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return True if the tensor is on GPU.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>Is the torchtt.TT on GPU or not.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_cuda(self):
    &#34;&#34;&#34;
    Return True if the tensor is on GPU.

    Returns:
        bool: Is the torchtt.TT on GPU or not.
    &#34;&#34;&#34;
    return all([c.is_cuda for c in self.core])</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.mprod"><code class="name flex">
<span>def <span class="ident">mprod</span></span>(<span>self, factor_matrices, mode)</span>
</code></dt>
<dd>
<div class="desc"><p>n-mode product.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>factor_matrices</code></strong> :&ensp;<code>torch.tensor</code> or <code>list[torch.tensor]</code></dt>
<dd>either a single matrix is directly provided or a list of matrices for product along multiple modes.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>int</code> or <code>list[int]</code></dt>
<dd>the mode for the product. If factor_matrices is a torch.tensor then mode is an integer and the multiplication will be performed along a single mode.
If factor_matrices is a list, the mode has to be list[int] of equal size.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Invalid arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the result</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mprod(self, factor_matrices, mode):
    &#34;&#34;&#34;
    n-mode product.

    Args:
        factor_matrices (torch.tensor or list[torch.tensor]): either a single matrix is directly provided or a list of matrices for product along multiple modes.
        mode (int or list[int]): the mode for the product. If factor_matrices is a torch.tensor then mode is an integer and the multiplication will be performed along a single mode.
                                 If factor_matrices is a list, the mode has to be list[int] of equal size.

    Raises:
        InvalidArguments: Invalid arguments.

    Returns:
        torchtt.TT: the result
    &#34;&#34;&#34;

    if isinstance(factor_matrices,list) and isinstance(mode, list):
        cores_new = [c.clone() for c in self.cores]
        for i in range(len(factor_matrices)):
            cores_new[mode[i]] =  tn.einsum(&#39;imjk,lj-&gt;imlk&#39;,cores_new[mode[i]],factor_matrices[i]) if self.is_ttm else tn.einsum(&#39;ijk,lj-&gt;ilk&#39;,cores_new[mode[i]],factor_matrices[i]) 
    elif isinstance(factor_matrices, tn.tensor) and isinstance(mode, int):
        cores_new = [c.clone() for c in self.cores]
        cores_new[mode] =  tn.einsum(&#39;imjk,lj-&gt;imlk&#39;,cores_new[mode],factor_matrices) if self.is_ttm else tn.einsum(&#39;ijk,lj-&gt;ilk&#39;,cores_new[mode],factor_matrices) 
    else:
        raise InvalidArguments(&#39;Invalid arguments.&#39;)
    
    return TT(cores_new)        </code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.norm"><code class="name flex">
<span>def <span class="ident">norm</span></span>(<span>self, squared=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the frobenius norm of a TT object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>squared</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>returns the square of the norm if True. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>the norm.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def norm(self,squared=False):
    &#34;&#34;&#34;
    Computes the frobenius norm of a TT object.

    Args:
        squared (bool, optional): returns the square of the norm if True. Defaults to False.

    Returns:
        torch.tensor: the norm.
    &#34;&#34;&#34;
    
    if any([c.requires_grad or c.grad_fn != None for c in self.cores]):
        norm = tn.tensor([[1.0]],dtype = self.cores[0].dtype, device=self.cores[0].device)
        
        if self.is_ttm:
            for i in range(len(self.N)):
                norm = tn.einsum(&#39;ab,aijm,bijn-&gt;mn&#39;,norm, self.cores[i], self.cores[i])
            norm = tn.squeeze(norm)
        else:
                       
            for i in range(len(self.N)):
                norm = tn.einsum(&#39;ab,aim,bin-&gt;mn&#39;,norm, self.cores[i], self.cores[i])
            norm = tn.squeeze(norm)
        if squared:
            return norm
        else:
            return tn.sqrt(tn.abs(norm))

    else:        
        d = len(self.cores)

        core_now = self.cores[0]
        for i in range(d-1):
            if self.is_ttm:
                mode_shape = [core_now.shape[1],core_now.shape[2]]
                core_now = tn.reshape(core_now,[core_now.shape[0]*core_now.shape[1]*core_now.shape[2],-1])
            else:
                mode_shape = [core_now.shape[1]]
                core_now = tn.reshape(core_now,[core_now.shape[0]*core_now.shape[1],-1])
                
            # perform QR
            Qmat, Rmat = QR(core_now)
                 
            # take next core
            core_next = self.cores[i+1]
            shape_next = list(core_next.shape[1:])
            core_next = tn.reshape(core_next,[core_next.shape[0],-1])
            core_next = Rmat @ core_next
            core_next = tn.reshape(core_next,[Qmat.shape[1]]+shape_next)
            
            # update the cores
            
            core_now = core_next
        if squared:
            return tn.linalg.norm(core_next)**2
        else:
            return tn.linalg.norm(core_next)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the full tensor as a numpy.array.
In case of a TTM, the result has the shape M1 x M2 x &hellip; x Md x N1 x N2 x &hellip; x Nd.
If it is involved in an AD graph, an error will occur.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>the full tensor in numpy.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy(self):
    &#34;&#34;&#34;
    Return the full tensor as a numpy.array.
    In case of a TTM, the result has the shape M1 x M2 x ... x Md x N1 x N2 x ... x Nd.
    If it is involved in an AD graph, an error will occur.
    
    Returns:
        numpy.array: the full tensor in numpy.
    &#34;&#34;&#34;
    return self.full().cpu().numpy()</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.qtt_to_tens"><code class="name flex">
<span>def <span class="ident">qtt_to_tens</span></span>(<span>self, original_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform a tensor back from QTT.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>original_shape</code></strong> :&ensp;<code>list</code></dt>
<dd>the original shape.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Original shape must be a list.</dd>
<dt><code>ShapeMismatch</code></dt>
<dd>Mode sizes do not match.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the folded tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def qtt_to_tens(self, original_shape):
    &#34;&#34;&#34;
    Transform a tensor back from QTT.

    Args:
        original_shape (list): the original shape.

    Raises:
        InvalidArguments: Original shape must be a list.
        ShapeMismatch: Mode sizes do not match.

    Returns:
        torchtt.TT: the folded tensor.
    &#34;&#34;&#34;
    
    if not isinstance(original_shape,list):
        raise InvalidArguments(&#34;Original shape must be a list.&#34;)

    core = None
    cores_new = []
    
    if self.is_ttm:
        pass
    else:
        k = 0
        for c in self.cores:
            if core==None:
                core = c
                so_far = core.shape[1]
            else:
                core = tn.einsum(&#39;...i,ijk-&gt;...jk&#39;,core,c)
                so_far *= c.shape[1]
            if so_far==original_shape[k]:
                core = tn.reshape(core,[core.shape[0],-1,core.shape[-1]])
                cores_new.append(core)
                core = None
                k += 1
        if k!= len(original_shape):
            raise ShapeMismatch(&#39;Mode sizes do not match.&#39;)
    return TT(cores_new)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.reduce_dims"><code class="name flex">
<span>def <span class="ident">reduce_dims</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Reduces the size 1 modes of the TT-object.
At least one mode should be larger than 1.</p>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce_dims(self):
    &#34;&#34;&#34;
    Reduces the size 1 modes of the TT-object.
    At least one mode should be larger than 1.

    Returns:
        None.
    &#34;&#34;&#34;
    
    # TODO: implement a version that reduces the rank also. by spliting the cores with modes 1 into 2 using the SVD.
    
    if self.is_ttm:
        cores_new = []
        
        for i in range(len(self.N)):
            
            if self.cores[i].shape[1] == 1 and self.cores[i].shape[2] == 1:
                if self.cores[i].shape[0] &gt; self.cores[i].shape[3] or i == len(self.N)-1:
                    # multiply to the left
                    if len(cores_new) &gt; 0:
                        cores_new[-1] = tn.einsum(&#39;ijok,kl-&gt;ijol&#39;,cores_new[-1], self.cores[i][:,0,0,:])
                    else: 
                        # there is no core to the left. Multiply right.
                        if i != len(self.N)-1:
                            self.cores[i+1] = tn.einsum(&#39;ij,jkml-&gt;ikml&#39;, self.cores[i][:,0,0,:],self.cores[i+1])
                        else:
                            cores_new.append(self.cores[i])
                        
                else:
                    # multiply to the right. Set the carry 
                    self.cores[i+1] = tn.einsum(&#39;ij,jkml-&gt;ikml&#39;,self.cores[i][:,0,0,:],self.cores[i+1])
                    
            else:
                cores_new.append(self.cores[i])
                
        # update the cores and ranks and shape
        self.N = []
        self.M = []
        self.R = [1]
        for i in range(len(cores_new)):
            self.N.append(cores_new[i].shape[2])
            self.M.append(cores_new[i].shape[1])
            self.R.append(cores_new[i].shape[3])
        self.cores = cores_new
    else:
        cores_new = []
        
        for i in range(len(self.N)):
            
            if self.cores[i].shape[1] == 1:
                if self.cores[i].shape[0] &gt; self.cores[i].shape[2] or i == len(self.N)-1:
                    # multiply to the left
                    if len(cores_new) &gt; 0:
                        cores_new[-1] = tn.einsum(&#39;ijk,kl-&gt;ijl&#39;,cores_new[-1], self.cores[i][:,0,:])
                    else: 
                        # there is no core to the left. Multiply right.
                        if i != len(self.N)-1:
                            self.cores[i+1] = tn.einsum(&#39;ij,jkl-&gt;ikl&#39;, self.cores[i][:,0,:],self.cores[i+1])
                        else:
                            cores_new.append(self.cores[i])
                            
                        
                else:
                    # multiply to the right. Set the carry 
                    self.cores[i+1] = tn.einsum(&#39;ij,jkl-&gt;ikl&#39;,self.cores[i][:,0,:],self.cores[i+1])
                    
            else:
                cores_new.append(self.cores[i])
        
        
        # update the cores and ranks and shape
        self.N = []
        self.R = [1]
        for i in range(len(cores_new)):
            self.N.append(cores_new[i].shape[1])
            self.R.append(cores_new[i].shape[2])
        self.cores = cores_new
                
                
    self.shape = [ (m,n) for m,n in zip(self.M,self.N) ] if self.is_ttm else [n for n in self.N] </code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.round"><code class="name flex">
<span>def <span class="ident">round</span></span>(<span>self, eps=1e-12, rmax=2048)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements the rounding operations within a given tolerance epsilon.
The maximum rank is also provided.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>the relative accuracy. Defaults to 1e-12.</dd>
<dt><strong><code>rmax</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the maximum rank. Defaults to 2048.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the result.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def round(self, eps=1e-12, rmax = 2048): 
    &#34;&#34;&#34;
    Implements the rounding operations within a given tolerance epsilon.
    The maximum rank is also provided.

    Args:
        eps (float, optional): the relative accuracy. Defaults to 1e-12.
        rmax (int, optional): the maximum rank. Defaults to 2048.

    Returns:
        torchtt.TT: the result.
    &#34;&#34;&#34;
    
    # rmax is not list
    if not isinstance(rmax,list):
        rmax = [1] + len(self.N)*[rmax] + [1]
        
    # call the round function
    tt_cores, R = round_tt(self.cores, self.R.copy(), eps, rmax,self.is_ttm)
    # creates a new TT and return it
    T = TT(tt_cores)
           
    return T</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.sum"><code class="name flex">
<span>def <span class="ident">sum</span></span>(<span>self, index=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Contracts a tensor in the TT format along the given indices and retuyrns the resulting tensor in the TT format.
If no index list is given, the sum over all indices is performed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>index</code></strong> :&ensp;<code>int</code> or <code>list[int]</code> or <code>None</code>, optional</dt>
<dd>the indices along which the summation is performed. None selects all of them. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Invalid index.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code> or <code>torch.tensor</code></dt>
<dd>the result.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sum(self,index = None):
    &#34;&#34;&#34;
    Contracts a tensor in the TT format along the given indices and retuyrns the resulting tensor in the TT format.
    If no index list is given, the sum over all indices is performed.

    Args:
        index (int or list[int] or None, optional): the indices along which the summation is performed. None selects all of them. Defaults to None.

    Raises:
        InvalidArguments: Invalid index.

    Returns:
        torchtt.TT or torch.tensor: the result.
    &#34;&#34;&#34;
    
    if index != None and isinstance(index,int):
        index = [index]
    if not isinstance(index,list) and index != None:
        raise InvalidArguments(&#39;Invalid index.&#39;)
         
    if index == None: 
        # the case we need to sum over all modes
        if self.is_ttm:
            C = tn.reduce_sum(self.cores[0],[0,1,2])
            for i in range(1,len(self.N)):
                C = tn.sum(tn.einsum(&#39;i,ijkl-&gt;jkl&#39;,C,self.cores[i]),[0,1])
            S = tn.sum(C)
        else:
            C = tn.sum(self.cores[0],[0,1])
            for i in range(1,len(self.N)):
                C = tn.sum(tn.einsum(&#39;i,ijk-&gt;jk&#39;,C,self.cores[i]),0)
            S = tn.sum(C)
    else:
        # we return the TT-tensor with summed indices
        cores = []
        
        if self.is_ttm:
            tmp = [1,2]
        else:
            tmp = [1]
            
        for i in range(len(self.N)):
            if i in index:
                C = tn.sum(self.cores[i], tmp, keepdim = True)
                cores.append(C)
            else:
                cores.append(self.cores[i])
                    
        S = TT(cores)
        S.reduce_dims()
        
    return S</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.t"><code class="name flex">
<span>def <span class="ident">t</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the transpoise of a given TT matrix.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>InvalidArguments</code></dt>
<dd>Has to TT matrix.</dd>
</dl>
<p>Returns:
torchtt.TT: the transpose.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def t(self):
    &#34;&#34;&#34;
    Returns the transpoise of a given TT matrix.
    
    Raises:
        InvalidArguments: Has to TT matrix.
        
    Returns: 
        torchtt.TT: the transpose. 
    &#34;&#34;&#34; 
    if not self.is_ttm:
        raise InvalidArguments(&#39;Has to TT matrix.&#39;)
        
    cores_new = [tn.permute(c,[0,2,1,3]) for c in self.cores]
    
    return TT(cores_new)</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.to"><code class="name flex">
<span>def <span class="ident">to</span></span>(<span>self, device=None, dtype=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Moves the TT instance to the given device with the given dtype.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code>, optional</dt>
<dd>The desired device. If none is provided, the device is the CPU. Defaults to None.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>torch.dtype</code>, optional</dt>
<dd>The desired dtype (torch.float64, torch.float32,&hellip;). If none is provided the dtype is not changed. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to(self, device = None, dtype = None):
    &#34;&#34;&#34;
    Moves the TT instance to the given device with the given dtype.

    Args:
        device (torch.device, optional): The desired device. If none is provided, the device is the CPU. Defaults to None.
        dtype (torch.dtype, optional): The desired dtype (torch.float64, torch.float32,...). If none is provided the dtype is not changed. Defaults to None.
    &#34;&#34;&#34;
    t = TT(None)
    t.N = self.N.copy()
    t.R = self.R.copy()
    t.is_ttm = self.is_ttm
    if self.is_ttm:
        t.M = self.M.copy()
    t.cores = [ c.to(device=device,dtype=dtype) for c in self.cores]

    return t</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.to_qtt"><code class="name flex">
<span>def <span class="ident">to_qtt</span></span>(<span>self, eps=1e-12, mode_size=2, rmax=2048)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a tensor to the QTT format: N1 x N2 x &hellip; x Nd -&gt; mode_size x mode_size x &hellip; x mode_size.
The product of the mode sizes should be a power of mode_size.
The tensor in QTT can be converted back using the qtt_to_tens() method.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>eps</code></strong> :&ensp;<code>flaot</code>,optional</dt>
<dd>the accuracy. Defaults to 1e-12.</dd>
<dt><strong><code>mode_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the size of the modes. Defaults to 2.</dd>
<dt><strong><code>rmax</code></strong> :&ensp;<code>int</code></dt>
<dd>the maximum rank. Defaults to 2048.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ShapeMismatch</code></dt>
<dd>Only quadratic TTM can be tranformed to QTT.</dd>
<dt><code>ShapeMismatch</code></dt>
<dd>Reshaping error: check if the dimensions are powers of the desired mode size.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the resulting reshaped tensor.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>import torchtt
x = torchtt.random([16,8,64,128],[1,2,10,12,1])
x_qtt = x.to_qtt()
print(x_qtt)
xf = x_qtt.qtt_to_tens(x.N) # a TT-rounding is recommended.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_qtt(self, eps = 1e-12, mode_size = 2, rmax = 2048):
    &#34;&#34;&#34;
    Converts a tensor to the QTT format: N1 x N2 x ... x Nd -&gt; mode_size x mode_size x ... x mode_size.
    The product of the mode sizes should be a power of mode_size.
    The tensor in QTT can be converted back using the qtt_to_tens() method.

    Args:
        eps (flaot,optional): the accuracy. Defaults to 1e-12.
        mode_size (int, optional): the size of the modes. Defaults to 2.
        rmax (int): the maximum rank. Defaults to 2048.
        

    Raises:
        ShapeMismatch: Only quadratic TTM can be tranformed to QTT.
        ShapeMismatch: Reshaping error: check if the dimensions are powers of the desired mode size.

    Returns:
        torchtt.TT: the resulting reshaped tensor.
        
    Examples:
        import torchtt
        x = torchtt.random([16,8,64,128],[1,2,10,12,1])
        x_qtt = x.to_qtt()
        print(x_qtt)
        xf = x_qtt.qtt_to_tens(x.N) # a TT-rounding is recommended.
        
    &#34;&#34;&#34;
   
    cores_new = []
    if self.is_ttm:
        shape_new = []
        for i in range(len(self.N)):
            if self.N[i]!=self.M[i]:
                raise ShapeMismatch(&#39;Only quadratic TTM can be tranformed to QTT.&#39;)
            if self.N[i]==mode_size**int(math.log(self.N[i],mode_size)):
                shape_new += [(mode_size,mode_size)]*int(math.log(self.N[i],mode_size))
            else:
                raise ShapeMismatch(&#39;Reshaping error: check if the dimensions are powers of the desired mode size:\r\ncore size &#39;+str(list(self.cores[i].shape))+&#39; cannot be reshaped.&#39;)
            
        result = reshape(self, shape_new, eps, rmax)
    else:
        for core in self.cores:
            if int(math.log(core.shape[1],mode_size))&gt;2:
                Nnew = [core.shape[0]*mode_size]+[mode_size]*(int(math.log(core.shape[1],mode_size))-2)+[core.shape[2]*mode_size]
                try:
                    core = tn.reshape(core,Nnew)
                except:
                    raise ShapeMismatch(&#39;Reshaping error: check if the dimensions care powers of the desired mode size:\r\ncore size &#39;+str(list(core.shape))+&#39; cannot be reshaped to &#39;+str(Nnew))
                cores,_ = to_tt(core,Nnew,eps,rmax,is_sparse=False)
                cores_new.append(tn.reshape(cores[0],[-1,mode_size,cores[0].shape[-1]]))
                cores_new += cores[1:-1]
                cores_new.append(tn.reshape(cores[-1],[cores[-1].shape[0],mode_size,-1]))
            else: 
                cores_new.append(core)
        result = TT(cores_new)
        
    return result</code></pre>
</details>
</dd>
<dt id="torchtt.torchtt.TT.to_ttm"><code class="name flex">
<span>def <span class="ident">to_ttm</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a TT-tensor to the TT-matrix format. In the tensor has the shape N1 x &hellip; x Nd, the result has the shape
N1 x &hellip; x Nd x 1 x &hellip; x 1.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.TT</code></dt>
<dd>the result</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_ttm(self):
    &#34;&#34;&#34;
    Converts a TT-tensor to the TT-matrix format. In the tensor has the shape N1 x ... x Nd, the result has the shape 
    N1 x ... x Nd x 1 x ... x 1.

    Returns:
        torch.TT: the result
    &#34;&#34;&#34;

    cores_new = [tn.reshape(c,(c.shape[0],c.shape[1],1,c.shape[2])) for c in self.cores]
    return TT(cores_new)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchtt" href="index.html">torchtt</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="torchtt.torchtt.bilinear_form" href="#torchtt.torchtt.bilinear_form">bilinear_form</a></code></li>
<li><code><a title="torchtt.torchtt.dot" href="#torchtt.torchtt.dot">dot</a></code></li>
<li><code><a title="torchtt.torchtt.elementwise_divide" href="#torchtt.torchtt.elementwise_divide">elementwise_divide</a></code></li>
<li><code><a title="torchtt.torchtt.emptyTT" href="#torchtt.torchtt.emptyTT">emptyTT</a></code></li>
<li><code><a title="torchtt.torchtt.emptyTTM" href="#torchtt.torchtt.emptyTTM">emptyTTM</a></code></li>
<li><code><a title="torchtt.torchtt.eye" href="#torchtt.torchtt.eye">eye</a></code></li>
<li><code><a title="torchtt.torchtt.kron" href="#torchtt.torchtt.kron">kron</a></code></li>
<li><code><a title="torchtt.torchtt.meshgrid" href="#torchtt.torchtt.meshgrid">meshgrid</a></code></li>
<li><code><a title="torchtt.torchtt.numel" href="#torchtt.torchtt.numel">numel</a></code></li>
<li><code><a title="torchtt.torchtt.ones" href="#torchtt.torchtt.ones">ones</a></code></li>
<li><code><a title="torchtt.torchtt.randn" href="#torchtt.torchtt.randn">randn</a></code></li>
<li><code><a title="torchtt.torchtt.random" href="#torchtt.torchtt.random">random</a></code></li>
<li><code><a title="torchtt.torchtt.rank1TT" href="#torchtt.torchtt.rank1TT">rank1TT</a></code></li>
<li><code><a title="torchtt.torchtt.reshape" href="#torchtt.torchtt.reshape">reshape</a></code></li>
<li><code><a title="torchtt.torchtt.zeros" href="#torchtt.torchtt.zeros">zeros</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchtt.torchtt.TT" href="#torchtt.torchtt.TT">TT</a></code></h4>
<ul class="two-column">
<li><code><a title="torchtt.torchtt.TT.apply_mask" href="#torchtt.torchtt.TT.apply_mask">apply_mask</a></code></li>
<li><code><a title="torchtt.torchtt.TT.clone" href="#torchtt.torchtt.TT.clone">clone</a></code></li>
<li><code><a title="torchtt.torchtt.TT.cpu" href="#torchtt.torchtt.TT.cpu">cpu</a></code></li>
<li><code><a title="torchtt.torchtt.TT.cuda" href="#torchtt.torchtt.TT.cuda">cuda</a></code></li>
<li><code><a title="torchtt.torchtt.TT.detach" href="#torchtt.torchtt.TT.detach">detach</a></code></li>
<li><code><a title="torchtt.torchtt.TT.fast_matvec" href="#torchtt.torchtt.TT.fast_matvec">fast_matvec</a></code></li>
<li><code><a title="torchtt.torchtt.TT.full" href="#torchtt.torchtt.TT.full">full</a></code></li>
<li><code><a title="torchtt.torchtt.TT.is_cuda" href="#torchtt.torchtt.TT.is_cuda">is_cuda</a></code></li>
<li><code><a title="torchtt.torchtt.TT.mprod" href="#torchtt.torchtt.TT.mprod">mprod</a></code></li>
<li><code><a title="torchtt.torchtt.TT.norm" href="#torchtt.torchtt.TT.norm">norm</a></code></li>
<li><code><a title="torchtt.torchtt.TT.numpy" href="#torchtt.torchtt.TT.numpy">numpy</a></code></li>
<li><code><a title="torchtt.torchtt.TT.qtt_to_tens" href="#torchtt.torchtt.TT.qtt_to_tens">qtt_to_tens</a></code></li>
<li><code><a title="torchtt.torchtt.TT.reduce_dims" href="#torchtt.torchtt.TT.reduce_dims">reduce_dims</a></code></li>
<li><code><a title="torchtt.torchtt.TT.round" href="#torchtt.torchtt.TT.round">round</a></code></li>
<li><code><a title="torchtt.torchtt.TT.sum" href="#torchtt.torchtt.TT.sum">sum</a></code></li>
<li><code><a title="torchtt.torchtt.TT.t" href="#torchtt.torchtt.TT.t">t</a></code></li>
<li><code><a title="torchtt.torchtt.TT.to" href="#torchtt.torchtt.TT.to">to</a></code></li>
<li><code><a title="torchtt.torchtt.TT.to_qtt" href="#torchtt.torchtt.TT.to_qtt">to_qtt</a></code></li>
<li><code><a title="torchtt.torchtt.TT.to_ttm" href="#torchtt.torchtt.TT.to_ttm">to_ttm</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>